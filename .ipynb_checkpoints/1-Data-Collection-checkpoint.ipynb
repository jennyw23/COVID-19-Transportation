{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6b1c0c7-4e3b-489b-96ec-3d7bae067675",
   "metadata": {},
   "source": [
    "# PM2 Data Collection<a href=\"#PM2-Data-Collection\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "### Jenny Wang<a href=\"#Jenny-Wang\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "Due Date: April 8, 2022\n",
    "\n",
    "### Overview:<a href=\"#Overview:\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "This notebook creates basic functions to parse the COVID-19 GitHub repo,\n",
    "get Tweet IDs from a text file, open tweet JSON from a Tweet ID, etc. It\n",
    "also does initial COVID-19+Transportation tasks by parsing our keywords\n",
    "data set and defining functions to fetch COVID-19+Transportation tweets.\n",
    "Lastly, these functions help make up the `general-collector.py` script\n",
    "which we use to scrape most of our data from the COVID-19 repo.\n",
    "\n",
    "### Instructions:<a href=\"#Instructions:\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "You should create two separate notebooks:\n",
    "\n",
    "1.  **for collecting data --\\> The data collection notebook shows how\n",
    "    you used APIs or web scraping to get the data (or part of it, if you\n",
    "    will rely on a bigger dataset from someone else)**\n",
    "2.  for exploring data --\\> Your data exploration notebook should have\n",
    "    code that generates various statistics: how many data points, what\n",
    "    does each datapoint look like, what kind of features it has, what\n",
    "    are some things you can say about these features, is there any\n",
    "    cleaning that you need to do, etc.\n",
    "\n",
    "If the data allows it, you can try to create visualizations using one of\n",
    "the visualization libraries that were introduced in WT1 (Matplotlib,\n",
    "Plotly, or Seaborn). These visualizations can be used in the paper, if\n",
    "they reveal interesting things about the data.\n",
    "\n",
    "### Table of Contents<a href=\"#Table-of-Contents\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "1.  [Parse COVID-19 Dataset](#1)\n",
    "    1.  [Access the COVID-19 dataset files](#a)\n",
    "    2.  [Get Tweet IDs from a text file](#b)\n",
    "2.  [Parse Tweet IDs for text body](#2)\n",
    "    1.  [Set up Twitter API keys and initialize necessary\n",
    "        libraries](#2a)\n",
    "    2.  ['Transform' keywords csv file and define functions for keyword\n",
    "        searching](#2b)\n",
    "    3.  [Save the date range of the transportation tweets saved in each\n",
    "        file](#2c)\n",
    "    4.  [Define functions to fetch and save COVID-19+Transportation\n",
    "        data](#2d)\n",
    "3.  [Finally, call the Twitter API](#3)\n",
    "4.  [Dehydrate `jsonl` files back to `txt` file of tweet IDs](#4)\n",
    "5.  [Update: Data Collection through `general-collector.py`\n",
    "    (4/7/2022)](#5)\n",
    "\n",
    "### My data collection involves a three-step process:<a href=\"#My-data-collection-involves-a-three-step-process:\"\n",
    "class=\"anchor-link\">¶</a>\n",
    "\n",
    "1.  Access the tweet ID text files from the [COVID-19\n",
    "    dataset](https://github.com/echen102/COVID-19-TweetIDs).\n",
    "2.  Combine tweet ID text files into larger documents to \"rehydrate\" the\n",
    "    tweet IDs into their full json form.\n",
    "3.  Filter COVID tweets by keywords related to transportation.\n",
    "\n",
    "Goal: create training dataset. No corresponding labels (this is\n",
    "unsupervised)\n",
    "\n",
    "\\(1\\) Parse COVID-19 Dataset\n",
    "\n",
    "#### Key Events<a href=\"#Key-Events\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "Using the [CDC COVID-19\n",
    "Timeline](https://www.cdc.gov/museum/timeline/covid19.html), we focus\n",
    "our data collection on key time periods in which tweets related to\n",
    "COVID-19 intersect with phrases related to transportation.\n",
    "\n",
    "Origins: December 2019 - January 2020\n",
    "\n",
    "-   December 12, 2019: Patients in Wuhan begin experiencing symptoms of\n",
    "    what later becomes known as COVID-19.\n",
    "-   January 20, 2020: CDC confirms the first U.S. laboratory-confirmed\n",
    "    case of COVID-19 in the U.S. from samples taken on January 18 in\n",
    "    Washington state.\n",
    "\n",
    "Early Pandemic: February 2020 - May 2020\n",
    "\n",
    "-   February 23, 2020: Italy becomes a global COVID-19 hotspot.\n",
    "-   [February 26,\n",
    "    2020](https://www.cdc.gov/media/releases/2020/t0225-cdc-telebriefing-covid-19.html):\n",
    "    CDC’s Dr. Nancy Messonnier, Incident Manager for the COVID-19\n",
    "    Response, holds a telebriefing. During the telebriefing she braces\n",
    "    the U.S. for the eventual community spread of the novel coronavirus\n",
    "    and states that the “disruption to everyday life may be severe.”\n",
    "-   March 11, 2020: The World Health Organization declares COVID-19 a\n",
    "    pandemic.\n",
    "-   March 13, 2020: President Donald J. Trump declares a nationwide\n",
    "    emergency.\n",
    "-   March 15, 2020: U.S. states begin to shut down to prevent the spread\n",
    "    of COVID-19. New York City public schools system (the largest school\n",
    "    system in the U.S., with 1.1 million students) shuts down, while\n",
    "    Ohio calls for restaurants and bars to close.\n",
    "-   March 28, 2020: White House extends social distancing measures until\n",
    "    the end of April 2020.\n",
    "-   April 3, 2020: At a White House press briefing, CDC announces new\n",
    "    mask wearing guidelines and recommends that all people wear a mask\n",
    "    when outside of the home.\n",
    "-   May 2, 2020: World Health Organization renews its emergency\n",
    "    declaration from three months prior calling the pandemic a global\n",
    "    health crisis.\n",
    "-   May 8, 2020: News media outlets report that top White House\n",
    "    officials shelve CDC “Guidance for Implementing the Opening Up\n",
    "    America Again Framework” that include detailed advice on how to\n",
    "    safely reopen the country.\n",
    "-   July 23, 2020: CDC releases new science-based resources and tolls\n",
    "    for school administrators, teachers, parents, guardians, and\n",
    "    caregivers for safe school reopening.\n",
    "\n",
    "COVID Vaccines Rollout: December 2020 - April 2021\n",
    "\n",
    "-   December 11, 2020: Food and Drug Administration issues an Emergency\n",
    "    Use Authorization (EUA) for the first COVID-19 vaccine – the\n",
    "    Pfizer-BioNTech COVID-19 vaccine.\n",
    "\n",
    "-   December 24, 2020: It is estimated that more than 1 million people\n",
    "    in the U.S. are vaccinated against COVID-19.\n",
    "\n",
    "-   March 8, 2021: CDC announces that fully vaccinated people can gather\n",
    "    indoors without masks.\n",
    "\n",
    "-   April 2, 2021: CDC announces fully vaccinated individuals can travel\n",
    "    safely domestically in the U.S. without a COVID test first.\n",
    "\n",
    "-   July 27, 2021: After a substantial upswing in cases due to the Delta\n",
    "    variant, CDC releases updated guidance for everyone in areas with\n",
    "    substantial or high transmission to wear a mask while indoors.\n",
    "\n",
    "-   November 29, 2021: CDC recommends that everyone over 18 years old\n",
    "    who received a Pfizer or Moderna vaccine receive a COVID-19 booster\n",
    "    shot 6 months after they are fully vaccinated.\n",
    "\n",
    "-   **March 10, 2022: At CDC’s recommendation, TSA extends the security\n",
    "    directive for mask use on public transportation and transportation\n",
    "    hubs for one month, through April 18th.**\n",
    "\n",
    "<https://www.cdc.gov/media/releases/2020/t0225-cdc-telebriefing-covid-19.html>\n",
    "\n",
    "In \\[1\\]:\n",
    "\n",
    "    import pandas as pd\n",
    "    import requests\n",
    "    import io\n",
    "    import os\n",
    "    import json\n",
    "    import datetime\n",
    "\n",
    "    # twitter API-related\n",
    "    import tweepy\n",
    "    from tqdm import tqdm\n",
    "    from twarc import Twarc\n",
    "\n",
    "Access the COVID-19 dataset files\n",
    "\n",
    "According to the COVID-19 dataset documentation, the Tweet-IDs are\n",
    "organized as follows:\n",
    "\n",
    "-   Tweet-ID files are stored in folders that indicate the year and\n",
    "    month of the collection (YEAR-MONTH).\n",
    "-   Individual Tweet-ID files contain a collection of Tweet IDs, and the\n",
    "    file names all follow the same structure, with a prefix\n",
    "    “coronavirus-tweet-id-” followed by the YEAR-MONTH-DATE-HOUR.\n",
    "\n",
    "The COVID-19 Tweet IDs are uploaded from 0:00-23:00, representing each\n",
    "hour in the day. Since some files may be missing (several hours in the\n",
    "day did not upload), we iterate over the files using the\n",
    "`YEAR-MONTH-DATE-HOUR` pattern, ignoring any URLs in which there is a\n",
    "`404 Not Found` Error.\n",
    "\n",
    "In \\[2\\]:\n",
    "\n",
    "    # get outer directories\n",
    "    base_url = 'https://github.com/echen102/COVID-19-TweetIDs/tree/master/'\n",
    "    time_range = ['2020-01', '2020-02', '2020-03', '2020-04', '2020-05', '2020-06', '2020-07', '2020-08', '2020-09', '2020-10', '2020-11', '2020-12', \n",
    "                '2021-01', '2021-02', '2021-03', '2021-04', '2021-05', '2021-06', '2021-07', '2021-08', '2021-09', '2021-10', '2021-11', '2021-12', \n",
    "                '2022-01', '2022-02', '2022-03', '2022-04']\n",
    "\n",
    "    # show the Tweet-ID `YEAR-MONTH` folders\n",
    "    for yyyyMM in time_range:\n",
    "        print(base_url+yyyyMM)\n",
    "\n",
    "    https://github.com/echen102/COVID-19-TweetIDs/tree/master/2020-01\n",
    "    https://github.com/echen102/COVID-19-TweetIDs/tree/master/2020-02\n",
    "    https://github.com/echen102/COVID-19-TweetIDs/tree/master/2020-03\n",
    "    https://github.com/echen102/COVID-19-TweetIDs/tree/master/2020-04\n",
    "    https://github.com/echen102/COVID-19-TweetIDs/tree/master/2020-05\n",
    "    https://github.com/echen102/COVID-19-TweetIDs/tree/master/2020-06\n",
    "    https://github.com/echen102/COVID-19-TweetIDs/tree/master/2020-07\n",
    "    https://github.com/echen102/COVID-19-TweetIDs/tree/master/2020-08\n",
    "    https://github.com/echen102/COVID-19-TweetIDs/tree/master/2020-09\n",
    "    https://github.com/echen102/COVID-19-TweetIDs/tree/master/2020-10\n",
    "    https://github.com/echen102/COVID-19-TweetIDs/tree/master/2020-11\n",
    "    https://github.com/echen102/COVID-19-TweetIDs/tree/master/2020-12\n",
    "    https://github.com/echen102/COVID-19-TweetIDs/tree/master/2021-01\n",
    "    https://github.com/echen102/COVID-19-TweetIDs/tree/master/2021-02\n",
    "    https://github.com/echen102/COVID-19-TweetIDs/tree/master/2021-03\n",
    "    https://github.com/echen102/COVID-19-TweetIDs/tree/master/2021-04\n",
    "    https://github.com/echen102/COVID-19-TweetIDs/tree/master/2021-05\n",
    "    https://github.com/echen102/COVID-19-TweetIDs/tree/master/2021-06\n",
    "    https://github.com/echen102/COVID-19-TweetIDs/tree/master/2021-07\n",
    "    https://github.com/echen102/COVID-19-TweetIDs/tree/master/2021-08\n",
    "    https://github.com/echen102/COVID-19-TweetIDs/tree/master/2021-09\n",
    "    https://github.com/echen102/COVID-19-TweetIDs/tree/master/2021-10\n",
    "    https://github.com/echen102/COVID-19-TweetIDs/tree/master/2021-11\n",
    "    https://github.com/echen102/COVID-19-TweetIDs/tree/master/2021-12\n",
    "    https://github.com/echen102/COVID-19-TweetIDs/tree/master/2022-01\n",
    "    https://github.com/echen102/COVID-19-TweetIDs/tree/master/2022-02\n",
    "    https://github.com/echen102/COVID-19-TweetIDs/tree/master/2022-03\n",
    "    https://github.com/echen102/COVID-19-TweetIDs/tree/master/2022-04\n",
    "\n",
    "In \\[5\\]:\n",
    "\n",
    "    # selected_time_range = ['2020-01', '2020-02']\n",
    "\n",
    "    def fetchAllTextFileURLs(time_range):\n",
    "        files = []\n",
    "        base_url = 'https://raw.githubusercontent.com/echen102/COVID-19-TweetIDs/master/'\n",
    "        prefix = 'coronavirus-tweet-id'\n",
    "        dates = list(range(1, 32)) # day 1 to 31\n",
    "        hours = list(range(24)) # hour 0 to 23\n",
    "        \n",
    "        for yyyyMM in time_range:\n",
    "            for date in dates:\n",
    "                date = f'0{date}' if date < 10 else f'{date}'\n",
    "                for hour in hours:\n",
    "                    hour = f'0{hour}' if hour < 10 else f'{hour}'\n",
    "                    fileURL = f'{base_url}{yyyyMM}/{prefix}-{yyyyMM}-{date}-{hour}.txt'\n",
    "                    files.append(fileURL)\n",
    "\n",
    "                    # print(fileURL)\n",
    "        \n",
    "        return files\n",
    "\n",
    "    # testFiles = fetchAllTextFileURLs(selected_time_range)\n",
    "    testFiles = fetchAllTextFileURLs(time_range)\n",
    "\n",
    "    print('Total files in Time Range:', len(testFiles))\n",
    "\n",
    "    Total files in Time Range: 20832\n",
    "\n",
    "Get Tweet IDs from a text file.\n",
    "\n",
    "The algorithm we used to generate the dataset files used the dataset's\n",
    "labels to will generate all dates/times between January 1, 2020 and\n",
    "April, 2022 if we set the time range as 1/2020 to 4/2022. Since the\n",
    "COVID-19's data collection did not begin until January 21 and known data\n",
    "collection gaps have occurred from then to now, we cannot rely on the\n",
    "URL being 100% valid.\n",
    "\n",
    "Thus, for each url generated, we call `getTweetIDsFrom(txtFile)` to\n",
    "check if the file exists. If it has a `404 File Not Found` error, then\n",
    "we ignore it. Otherwise, we run through the `.txt` file to retrieve all\n",
    "the tweet IDs.\n",
    "\n",
    "In \\[6\\]:\n",
    "\n",
    "    def getTweetIDsFrom(txtFile):\n",
    "        '''\n",
    "        Retrieves tweet IDs from the text file's URL and stores in list\n",
    "        '''\n",
    "        tweetList = []\n",
    "        response = requests.get(txtFile) # requests raw file from Github\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            content = response.content.decode('utf-8')\n",
    "            tweetList = content.split()\n",
    "            \n",
    "        return tweetList\n",
    "\n",
    "In \\[7\\]:\n",
    "\n",
    "    # sample URLS for testing\n",
    "    url2020 = testFiles[535]\n",
    "    url2021 = testFiles[1000]\n",
    "\n",
    "    print(url2020)\n",
    "    tweets2020 = getTweetIDsFrom(url2020)\n",
    "    print(url2021)\n",
    "    tweets2021 = getTweetIDsFrom(url2021)\n",
    "\n",
    "    print(\"January 23, 2020, at 7am :\",len(tweets2020))\n",
    "    print(\"February 11, 2021, at 4pm:\", len(tweets2021))\n",
    "\n",
    "    https://raw.githubusercontent.com/echen102/COVID-19-TweetIDs/master/2020-01/coronavirus-tweet-id-2020-01-23-07.txt\n",
    "    https://raw.githubusercontent.com/echen102/COVID-19-TweetIDs/master/2020-02/coronavirus-tweet-id-2020-02-11-16.txt\n",
    "    January 23, 2020, at 7am : 2006\n",
    "    February 11, 2021, at 4pm: 41281\n",
    "\n",
    "From our two tests, we see that the volume of tweets increased\n",
    "significantly as recognition of the COVID-19 pandemic spread. In late\n",
    "January 2020, we see that there are just \\~2000 hourly tweets about\n",
    "COVID-19, while in mid February 2021, there are about \\~41,000 hourly\n",
    "tweets.\n",
    "\n",
    "\\(2\\) Parse Tweet IDs for text body.\n",
    "\n",
    "We are now ready to begin our search for transportation data within the\n",
    "COVID-19 dataset. We created a \"keywords\" list of possible phrases and\n",
    "words related to transportation and mobility. The list is not\n",
    "exhaustive, but seeks to include phrases that are not directly related\n",
    "to my analysis to ensure we have as robust a dataset as possible.\n",
    "\n",
    "`Twarc` is a Python library that collects and archives Twitter data via\n",
    "the Twitter API. The benefit of `Twarc` is that it contains two methods,\n",
    "called `hydrate` and `dehydrate` which can generate tweet JSON from a\n",
    "`.txt` file of tweet IDs and generate tweet IDs from a file of tweets.\n",
    "\n",
    "In our analysis, we will first generate json files of\n",
    "`COVID-19+transportation` tweets by iterating through the COVID-19\n",
    "dataset. After generating json files capped \\~1000 tweets, we will use\n",
    "Twarc to `dehydrate` the tweets into a compact `.txt` file.\n",
    "\n",
    "Set up Twitter API keys and initialize necessary libraries\n",
    "\n",
    "In \\[8\\]:\n",
    "\n",
    "    # ****Set up to access Twitter API****\n",
    "\n",
    "    # Assign developer keys\n",
    "    consumer_key='Hdzqjxf3mVyw5DQLUISQO8Dsz'\n",
    "    consumer_secret='9NN8pQz1gKfk8wIy8VnPXM2TWtvmvnW3n19rAavuo5MGiirDey'\n",
    "    access_token='1345900794625847301-pPqmLUdewBlbRx5awibCmvajKh6qnK'\n",
    "    access_token_secret='bVVGLYrMxqssj1RkEUigABAd5mBZ9i4RL1nP6j6tbeDmm'\n",
    "      \n",
    "    # authorization of consumer key and consumer secret\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "      \n",
    "    # set access to user's access key and access secret \n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "      \n",
    "    # calling the api \n",
    "    api = tweepy.API(auth)\n",
    "\n",
    "'Transform' keywords csv file and define functions for keyword searching\n",
    "\n",
    "In \\[11\\]:\n",
    "\n",
    "    # Transform keywords list\n",
    "\n",
    "    keywords = pd.read_csv('./cs315_keywords.csv')\n",
    "    keywords.head()\n",
    "    keywords['public transport']=keywords['public transport'].str.lower()\n",
    "    keywords['motorized']=keywords['motorized'].str.lower()\n",
    "    keywords['non-motorized']=keywords['non-motorized'].str.lower()\n",
    "\n",
    "    transit = keywords['public transport'].tolist()\n",
    "    motorized = keywords['motorized'].tolist()\n",
    "    nonmotorized = keywords['non-motorized'].tolist()\n",
    "\n",
    "    keywords = set(transit + motorized + nonmotorized)\n",
    "\n",
    "    print(keywords)\n",
    "    print(len(keywords))\n",
    "\n",
    "    {nan, 'red line', 'buick', 'bike', 'chevrolet', 'monorail', 'autos', 'mass transit', 'rail', 'walks', 'automobiles', 'light rail', 'traffic', 'chrysler', 'jog', 'ford', 'muni', 'road', 'electric scooter', 'fiat', 'nissan', 'railroads', 'streetcars', 'mazda', 'vehicles', 'gasoline', 'silver line', 'cars', 'railway', 'scooters', 'kia', 'bart', 'bicycling', 'caltrain', 'jeep', 'lexus', 'rollerskate', 'commuter', 'route', 'rideshare', 'run', 'rollerblading', 'mta', 'roads', 'transportation', 'uber', 'metropolitan', 'hyundai', 'streetcar', 'railroad', 'commute', 'bus', 'walking', 'shuttle', 'amtrak', 'e-scooter', 'auto', 'bicycles', 'diesel', 'cable car', 'toyota', 'jogging', 'blue line', 'station', 'mass transportation', 'congestion', 'rides', 'honda', 'trolley', 'bicycle', 'engine', 'lincoln', 'commuter rail', 'highway', 'subaru', 'roadway', 'gas prices', 'lane', 'carpooling', 'workout', 'carpool', 'green line', 'bikers', 'walkers', 'running', 'exercise', 'transport', 'subway', 'rapid transit', 'metrorail', 'buses', 'pedestrian', 'dodge', 'line', 'mbta', 'rollerblade', 'cycle', 'car', 'acura', 'mitsubishi', 'cycling', 'scooter', 'cycles', 'transit', 'vehicle', 'lyft', 'walk', 'automobile', 'heavy rail', 'metro', 'infiniti', 'freeway'}\n",
    "    112\n",
    "\n",
    "In \\[10\\]:\n",
    "\n",
    "    def containsKeyword(text, keywords):\n",
    "        '''\n",
    "        Checks if a word in the tweet body is located in the keywords list.\n",
    "        '''    \n",
    "        \n",
    "        loweredText = [word.lower() for word in text.split()]\n",
    "        textSet = set(loweredText) \n",
    "        \n",
    "        intersection = textSet & keywords\n",
    "        \n",
    "        return False if not intersection else True\n",
    "\n",
    "    print(containsKeyword(\"hi there car subaru drive\", keywords))\n",
    "    print(containsKeyword(\"Creating a Grocery List Manager Using Angular, Part 1: Add &amp; Display Items https://t.co/xFox78juL1 #Angular\", keywords))\n",
    "\n",
    "    True\n",
    "    False\n",
    "\n",
    "Save the date range of the transportation tweets saved in each file\n",
    "\n",
    "First, we generate functions to patternize the file names. Each `.jsonl`\n",
    "file will be prefixed with `covid-mobility-tweets-starting` followed by\n",
    "the date of the first tweet in the file.\n",
    "\n",
    "Template: `covid-mobility-tweets-starting-YYYY-MM-DD.jsonl`\n",
    "\n",
    "Example: `covid-mobility-tweets-starting-2020-05-01.jsonl`\n",
    "\n",
    "In \\[18\\]:\n",
    "\n",
    "    # test datetime object format\n",
    "    date_time_str = \"Wed Oct 10 20:19:24 +0000 2018\"\n",
    "    date_time_obj = datetime.datetime.strptime(date_time_str, '%a %b %d %H:%M:%S %z %Y')\n",
    "    time = date_time_obj.strftime(\"%H:%M:%S\")\n",
    "    print(\"***TESTS***\")\n",
    "    print(\"Year Month Day\", date_time_obj.year, date_time_obj.month, date_time_obj.day, time)\n",
    "\n",
    "    # note: I removed some keys for brevity\n",
    "    sampleTweet = {\n",
    "        'created_at': 'Fri May 01 00:11:52 +0000 2020',\n",
    "     'id': 1256013431855108096,\n",
    "     'id_str': '1256013431855108096',\n",
    "     'text': 'RT @VEJA: Covid-19 avança nas favelas — mas ruas continuam cheias com gente que não pode parar e se proteger https://t.co/rPk8eequN5 https:…',\n",
    "     'truncated': False,\n",
    "     'entities': {'hashtags': [],\n",
    "      'symbols': [],\n",
    "      'user_mentions': [{'screen_name': 'VEJA',\n",
    "        'name': 'VEJA',\n",
    "        'id': 17715048,\n",
    "        'id_str': '17715048',\n",
    "        'indices': [3, 8]}],\n",
    "      'urls': [{'url': 'https://t.co/rPk8eequN5',\n",
    "        'expanded_url': 'https://veja.abril.com.br/brasil/o-coronavirus-chega-a-favela/',\n",
    "        'display_url': 'veja.abril.com.br/brasil/o-coron…',\n",
    "        'indices': [109, 132]}]},\n",
    "     'source': '<a href=\"https://mobile.twitter.com\" rel=\"nofollow\">Twitter Web App</a>',\n",
    "     'geo': None,\n",
    "     'coordinates': None,\n",
    "     'place': None,\n",
    "     'contributors': None,\n",
    "    }\n",
    "\n",
    "    # Test helper functions\n",
    "    print(\"Tweet File Name: \", formatFileName([sampleTweet]))\n",
    "    print(fetchStartAndEndDate([sampleTweet], 1))\n",
    "\n",
    "    ***TESTS***\n",
    "    Year Month Day 2018 10 10 20:19:24\n",
    "    Tweet File Name:  covid-mobility-tweet-starting-2020-5-1-0.jsonl\n",
    "    Date Range: 2020-5-1-0 00:11:52 to 2020-5-1 00:11:52\n",
    "\n",
    "In \\[19\\]:\n",
    "\n",
    "    # helper functions used for saveTransportationTweets()\n",
    "    def fetchStartAndEndDate(tweetArr, count):\n",
    "        '''\n",
    "        Returns the first and last tweet in the dataset\n",
    "        in the format YEAR-MONTH-DATE-HOUR to YEAR-MONTH-DATE-HOUR \n",
    "        '''\n",
    "        startTimeStr = tweetArr[0][\"created_at\"]\n",
    "        s = datetime.datetime.strptime(startTimeStr, '%a %b %d %H:%M:%S %z %Y')\n",
    "        sTime = s.strftime(\"%H:%M:%S\")\n",
    "        # print(\"s\",sTime)\n",
    "        endTimeStr = tweetArr[count-1][\"created_at\"]\n",
    "        e = datetime.datetime.strptime(endTimeStr, '%a %b %d %H:%M:%S %z %Y')\n",
    "        eTime = e.strftime(\"%H:%M:%S\")\n",
    "        # print(\"e\",eTime)\n",
    "\n",
    "        dateRange = f'Date Range: {s.year}-{s.month}-{s.day}-{s.hour} {sTime} to {e.year}-{e.month}-{e.day} {eTime}'\n",
    "        \n",
    "        return dateRange \n",
    "\n",
    "    def formatFileName(tweetArr):\n",
    "        '''\n",
    "        Formats the file name of the json files according to the first and last tweet in the dataset\n",
    "        using the prefix \"covid-mobility-tweet-starting\" followed by YEAR-MONTH-DATE~HH-mm-ss.\n",
    "        '''\n",
    "        startTimeStr = tweetArr[0][\"created_at\"]\n",
    "        s = datetime.datetime.strptime(startTimeStr, '%a %b %d %H:%M:%S %z %Y')\n",
    "        sTime = s.strftime(\"%H-%M-%S\")\n",
    "\n",
    "        base_path = f'covid-mobility-tweet-starting-{s.year}-{s.month}-{s.day}'\n",
    "        filename_0 = base_path+'-0'\n",
    "        \n",
    "        if not os.path.exists(f'{filename_0}.jsonl'): # path with this date does not exist yet\n",
    "            return filename_0+'.jsonl'\n",
    "        else:    \n",
    "            # save file as the next `filename-i` filepath for this date\n",
    "            i = 1\n",
    "            while os.path.exists(f'{base_path}-{i}.jsonl'):\n",
    "                i+=1\n",
    "            filename_i = f'{base_path}-{i}'\n",
    "        \n",
    "            return filename_i+'.jsonl'\n",
    "            \n",
    "\n",
    "    def writeToJsonlFile(tweetArr, dateRange, filename):\n",
    "        '''\n",
    "        Writes a list of tweets to a given .jsonl file. Note that .jsonl and .json differ in structure.\n",
    "        '''\n",
    "        filepath = './covid-mobility-tweets/'+filename\n",
    "        with open(filepath, 'w') as outfile:\n",
    "            outfile.write(f'{{{dateRange}}}\\n') # dumps the date range in json format\n",
    "            for entry in tweetArr:\n",
    "                json.dump(entry, outfile)\n",
    "                outfile.write('\\n')\n",
    "\n",
    "    formatFileName([sampleTweet])\n",
    "\n",
    "Out\\[19\\]:\n",
    "\n",
    "    'covid-mobility-tweet-starting-2020-5-1-0.jsonl'\n",
    "\n",
    "The next few functions make up the core of the data collection. The\n",
    "`saveTransportationTweets()` function iterates through the list of\n",
    "tweets given an array of tweet IDs, adding tweets related to our\n",
    "transportation keywords to an transportation tweet array. When the size\n",
    "of the transportation tweet array reaches 1000, we save the file as a\n",
    "`.jsonl` file and reiterate through the process.\n",
    "\n",
    "Define functions to fetch and save `COVID-19+Transportation` data.\n",
    "\n",
    "In \\[20\\]:\n",
    "\n",
    "    def saveTransportationTweets(tweetIDs, keywords):\n",
    "        '''\n",
    "        Iterates through list of tweets and identifies tweets containing keywords. \n",
    "        \n",
    "        When the number of transportation tweets surpasses 1000, save the current tweets \n",
    "        as a json file and reset the json variable.\n",
    "        '''\n",
    "        tweetArr = []\n",
    "        count = 0\n",
    "        for tweet in tweetIDs:\n",
    "            # save json file and create new one\n",
    "            if count == 1000:\n",
    "                dateRange = fetchStartAndEndDate(tweetArr, count) # save date of first and last tweet\n",
    "                filename = formatFileName(tweetArr) # format file name\n",
    "                writeToJsonlFile(tweetArr, dateRange, filename) # save json as a json file\n",
    "                # reset 'global' vars\n",
    "                tweetArr = []\n",
    "                count = 0\n",
    "            try:\n",
    "                fetchedTweet = api.get_status(tweet)\n",
    "                \n",
    "                # search for keywords in tweet text\n",
    "                if containsKeyword(fetchedTweet.text, keywords):\n",
    "                    tweetArr.append(fetchedTweet._json)\n",
    "                    count += 1 # maintain counter to know when the json file reaches 1000 COVID-19+Transportation tweets\n",
    "                    print(f\"COVID-19+Transportation Tweet #{count}\\n\" + fetchedTweet.text)\n",
    "\n",
    "            except Exception as e:\n",
    "                # print(\"Exception Thrown: \", e)\n",
    "                continue\n",
    "        \n",
    "        # if loop exits befor count == 1000\n",
    "        if count > 0:\n",
    "            dateRange = fetchStartAndEndDate(tweetArr, count) # save date of first and last tweet\n",
    "            filename = formatFileName(tweetArr) # format file name\n",
    "            writeToJsonlFile(tweetArr, dateRange, filename) # save json as a json file    \n",
    "\n",
    "Finally, it's time to call the Twitter API\n",
    "\n",
    "Using the functions we defined above, we are now ready to call\n",
    "`saveTransportationTweets()` on the COVID-19 dataset files.\n",
    "\n",
    "In \\[21\\]:\n",
    "\n",
    "    time_range = ['2020-01', '2020-02', '2020-03', '2020-04', '2020-05', '2020-06', '2020-07', '2020-08', '2020-09', '2020-10', '2020-11', '2020-12', \n",
    "                '2021-01', '2021-02', '2021-03', '2021-04', '2021-05', '2021-06', '2021-07', '2021-08', '2021-09', '2021-10', '2021-11', '2021-12', \n",
    "                '2022-01', '2022-02', '2022-03']\n",
    "\n",
    "    covidDataset = fetchAllTextFileURLs(time_range)\n",
    "    covidDataset = covidDataset[502:] # data collection starts from 1/21/2020 at 22:00.\n",
    "    print('Total files in COVID-10 Dataset:', len(covidDataset))\n",
    "    # show first 5 txt files\n",
    "    print('The first 3 automatically generated files:', covidDataset[:3])\n",
    "\n",
    "    Total files in COVID-10 Dataset: 19586\n",
    "    The first 3 automatically generated files: ['https://raw.githubusercontent.com/echen102/COVID-19-TweetIDs/master/2020-01/coronavirus-tweet-id-2020-01-21-22.txt', 'https://raw.githubusercontent.com/echen102/COVID-19-TweetIDs/master/2020-01/coronavirus-tweet-id-2020-01-21-23.txt', 'https://raw.githubusercontent.com/echen102/COVID-19-TweetIDs/master/2020-01/coronavirus-tweet-id-2020-01-22-00.txt']\n",
    "\n",
    "The following lines of code do\n",
    "\n",
    "-   selects several .txt files to extract tweets from\n",
    "-   extracts tweet from each .txt file\n",
    "-   runs `saveTransportationTweets()` which rehydrates the Tweet object\n",
    "    and saves the relevant tweets in .jsonl files\n",
    "    -   we define 'relevant' using the [keywords](#2b) defined earlier.\n",
    "\n",
    "In \\[22\\]:\n",
    "\n",
    "    # selectedRange = covidDataset[8:15] # note that 1/21 happens at 24 * 21 = 504\n",
    "    # for file in selectedRange:\n",
    "    #     tweetIDs = getTweetIDsFrom(file)\n",
    "    #     if len(tweetIDs) > 0:\n",
    "    #         print('tweetIDs found')\n",
    "    #         saveTransportationTweets(tweetIDs, keywords)\n",
    "\n",
    "Dehydrate `jsonl` files back to `txt` file of tweet IDs\n",
    "\n",
    "Now that we have our dataset of `COVID-19+Transportation` tweets, we can\n",
    "test out Twarc's `dehydrate` function to compress the tweets into just\n",
    "tweet IDs. Twarc is also a command line too, which means we can run\n",
    "\n",
    "    twarc dehydrate covid-mobility-tweet-starting-2020-1-21.jsonl > covid-mobility-tweet-starting-2020-1-21.txt\n",
    "\n",
    "by utilizing the `os` library. According to the documentation, Twarc\n",
    "generates a text file containing the tweet IDs it finds based on the key\n",
    "`id_str`. The [source\n",
    "code](https://twarc-project.readthedocs.io/en/latest/api/client/) (for\n",
    "the Client method) is written below:\n",
    "\n",
    "    def dehydrate(self, iterator):\n",
    "    \"\"\"\n",
    "    Pass in an iterator of tweets' JSON and get back an iterator of the\n",
    "    IDs of each tweet.\n",
    "    \"\"\"\n",
    "    for line in iterator:\n",
    "        try:\n",
    "            yield json.loads(line)[\"id_str\"]\n",
    "        except Exception as e:\n",
    "            log.error(\"uhoh: %s\\n\" % e)\n",
    "\n",
    "In \\[23\\]:\n",
    "\n",
    "    # Initialize Twarc; not currently used in client\n",
    "    twarc = Twarc()\n",
    "\n",
    "In \\[26\\]:\n",
    "\n",
    "    # Test dehydrate on a sample .jsonl file\n",
    "    os.system('twarc dehydrate covid-mobility-tweet-starting-2020-1-21.jsonl > covid-mobility-tweet-starting-2020-1-21.txt')\n",
    "\n",
    "In \\[25\\]:\n",
    "\n",
    "    def findfile(name, path):\n",
    "        '''\n",
    "        Finds file in directory\n",
    "        '''\n",
    "        for dirpath, dirname, filename in os.walk(path):\n",
    "            if name in filename:\n",
    "                return os.path.join(dirpath, name)\n",
    "\n",
    "    filepath = findfile('covid-mobility-tweet-starting-2020-1-21.txt', \"./\")\n",
    "    print(filepath, '\\n')\n",
    "\n",
    "    with open(filepath, 'r') as file:\n",
    "        print(f\"Tweet IDs in {filepath}\")\n",
    "        for line in file:\n",
    "            print(line)\n",
    "\n",
    "    ./covid-mobility-tweet-starting-2020-1-21.txt \n",
    "\n",
    "    Tweet IDs in ./covid-mobility-tweet-starting-2020-1-21.txt\n",
    "\n",
    "Update: Data Collection through `general-collector.py` (4/7/2022)\n",
    "\n",
    "To collect data on the remote CS server, I reformatted my Jupyter\n",
    "Notebook into a Python script. I've been using the `nohup` command on\n",
    "Linux to constantly collect data from the COVID-19 dataset. Because of\n",
    "the difference between the notebook version and the script, I have had\n",
    "to make a few changes to this notebook, particularly in relation to the\n",
    "naming conventions. I've also uploaded my data collection script\n",
    "(`dataCollection.py`) in my `finalproject` folder.\n",
    "\n",
    "When creating new files:\n",
    "\n",
    "1.  Follow conventions to generate python file\n",
    "    `collect-MMDDYYYY-MMDDYYYY.py`\n",
    "2.  Create folder with the same format `collect-MMDDYYYY-MMDDYYYY`\n",
    "3.  Copy and older python file. Change the following:\n",
    "    1.  Ctrl-f `collect-`. Change two log files and the folder path to\n",
    "        the proper name\n",
    "    2.  Ctrl-f `.log\"`. Check two log files have been changed\n",
    "    3.  Ctrl-f `selectedRange`. Change the selectedRange to the date\n",
    "        range we're collecting from\n",
    "\n",
    "Date Ranges currently being collected from:\n",
    "\n",
    "-   3/11/2020 - 3/27/2020\n",
    "\n",
    "-   3/28/2020 - 5/1/2020\n",
    "\n",
    "-   5/2/2020 - 5/8/2020\n",
    "\n",
    "-   7/23/2020 - 8/30/2020\n",
    "\n",
    "-   11/24/2020 - 11/31/2020\n",
    "\n",
    "-   12/11/2020 - 12/25/2020\n",
    "\n",
    "In \\[10\\]:\n",
    "\n",
    "    # Call the general collector script\n",
    "    import os\n",
    "\n",
    "    def startGeneralCollector(collect_dateRange, startIndex, endIndex):\n",
    "        scriptCommand = '../miniconda3/bin/python3 collection-scripts/general-collector.py'\n",
    "        os.system(f'{scriptCommand} --collect_dateRange {collect_dateRange} --startIndex {startIndex} --endIndex {endIndex}')\n",
    "\n",
    "In \\[1\\]:\n",
    "\n",
    "    # startGeneralCollector('collect-06072021-06132021', 12290, 12458)\n",
    "\n",
    "I used [this\n",
    "spreadsheet](https://docs.google.com/spreadsheets/d/1hKOxDP_hTEeD7rBBV-W_REfw2Dt1pYwiJwfYwSy5u8Q/edit?usp=sharing)\n",
    "to keep track of the data collection periods and ensure I did not double\n",
    "collect data. There was no time to waste on silly mistakes (although I\n",
    "sure did make a few...)! The `general-collector` script was very helpful\n",
    "in streamlining the data collection process. The abstraction +\n",
    "spreadsheet combined allowed me to work 10x faster!\n",
    "\n",
    "## Other tests \\[can ignore\\]<a href=\"#Other-tests-%5Bcan-ignore%5D\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "As mentioned previously, our script creates \"urls\" between 1/2020 and\n",
    "the current month (4/2022) from day 1 to 31 and hour 0 to 23. We used\n",
    "this to simplify the url generation process when accessing the COVID-19\n",
    "Datset Github Repo. Because not all months have 31 days, and not all\n",
    "hours have collected data, we have an additional check that ensures we\n",
    "ignore false links.\n",
    "\n",
    "In \\[15\\]:\n",
    "\n",
    "    # Get an idea of the month to month breakdown\n",
    "\n",
    "    # Data collection for the mont of January, 2020 ran from 1/21/2020 to 1/31/2020, \n",
    "    # the 502th index to the 743 index of our covid dataset\n",
    "    print(\"January 2020\", covidDataset[0].split('-')[6:], covidDataset[241].split('-')[6:])\n",
    "\n",
    "    # We iterate over all 24 hours and all '31' days (regardless of month)\n",
    "    # To find the entire month of data, we look for covidDataset[744(X-2)+241, 744(X-1)+241+1] where X = month #\n",
    "    print(\"February 2020\", covidDataset[242].split('-')[6:], covidDataset[985].split('-')[6:])\n",
    "\n",
    "    print(\"March 2020\", covidDataset[986].split('-')[6:], covidDataset[1729].split('-')[6:])\n",
    "\n",
    "    print(\"April 2020\", covidDataset[1730].split('-')[6:], covidDataset[2473].split('-')[6:])\n",
    "\n",
    "    January 2020 ['2020', '01', '21', '22.txt'] ['2020', '01', '31', '23.txt']\n",
    "    February 2020 ['2020', '02', '01', '00.txt'] ['2020', '02', '31', '23.txt']\n",
    "    March 2020 ['2020', '03', '01', '00.txt'] ['2020', '03', '31', '23.txt']\n",
    "    April 2020 ['2020', '04', '01', '00.txt'] ['2020', '04', '31', '23.txt']\n",
    "\n",
    "In \\[28\\]:\n",
    "\n",
    "    covidDataset[78]\n",
    "\n",
    "Out\\[28\\]:\n",
    "\n",
    "    'https://raw.githubusercontent.com/echen102/COVID-19-TweetIDs/master/2020-01/coronavirus-tweet-id-2020-01-25-04.txt'\n",
    "\n",
    "In \\[69\\]:\n",
    "\n",
    "    print(len(covidDataset))\n",
    "    print(covidDataset[7922])\n",
    "    # save the second value bc it's index-1\n",
    "    print(covidDataset[8281:8282])\n",
    "\n",
    "    19586\n",
    "    https://raw.githubusercontent.com/echen102/COVID-19-TweetIDs/master/2020-12/coronavirus-tweet-id-2020-12-11-00.txt\n",
    "    ['https://raw.githubusercontent.com/echen102/COVID-19-TweetIDs/master/2020-12/coronavirus-tweet-id-2020-12-25-23.txt']\n",
    "\n",
    "In \\[ \\]:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
