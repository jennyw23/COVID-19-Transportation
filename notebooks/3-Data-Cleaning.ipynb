{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a166e62f",
   "metadata": {},
   "source": [
    "# Part 3 - Data Cleaning\n",
    "\n",
    "## Overview\n",
    "This notebook utilizes the functions we created in the \"Data Exploration\" notebook through the module `DataRetrieval`. Our goal in this notebook is to combine all the hourly tweet files into larger documents. We will save the collection tweets in files (1) by day, (2) by season (i.e. spring, summer, winter, fall), and (3) by the whole time range.\n",
    "\n",
    "After the data cleaning step, we move on to preprocessing, which is in the notebook `PM3 - Data Preprocessing`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d278113e",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "1. [Motivation](#1)\n",
    "    1. [Save collection tweets into larger files to prepare for large data analysis](#1a)\n",
    "    2. [Open the .jsonl file containing all transportation tweets ](#1b)\n",
    "    3. [Observe some of the text data](#1c)\n",
    "    4. [Dehydrate and rehydrate tweets to obtain full_text field](#1d)\n",
    "    5. [Observe tweet text after rehydrating](#1e)\n",
    "2. [Creating a tweet classifier](#2)\n",
    "    1. [Classifier to remove Words With Multiple Meanings](#2a)\n",
    "    2. [Observe some irrelevant tweets to see if we're capturing accurate information](#2b)\n",
    "    3. [Create a classifier which removes non-English tweets](#2c)\n",
    "3. [Store relevant tweet IDs in .txt file ](#3)\n",
    "    1. [Rehydrate our relevant tweets](#3a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f269ed",
   "metadata": {},
   "source": [
    "## Motivation <a href name id=\"1\">\n",
    "Now that I have collected a sizeable amount of data, I want to clean some of the tweets that do not contain transportation relevant tweets which I found in PM2:\n",
    "\n",
    "    Another issue is that our transportation keywords captured more than just mobility/transportation tweets. The word \"line\" is found quite a bit, but rarely relates to transportation. For example, Guess this is the new party line cause it sure ain't this thing called \"truth.\" https://t.co/rkB5Gwzal4 uses the word \"line\" in relation to political party lines. My exploration into other files also shows that line is rarely used in the transportation context. Even so, I think it's more important to capture this word in case transportation tweets are found. We will handle any saved tweets unrelated to transportation after the initial data collection phase (during preprocessing).\n",
    "\n",
    "    From other files, it also appears that some of these tweets may be duplicates from the same individual. I noticed this especially on 1/24/2020. It's probably a good idea to do some filtering in case those are spam accounts or bots which could bias our analysis. \n",
    "    \n",
    "In this notebook, I will tackle this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50d623c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import sys\n",
    "sys.path.append(\".\")\n",
    "import helper.DataRetrieval as dr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7e38439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example of using DataRetrieval module\n",
    "# # Gets a list of all text files in COVID-19 dataset\n",
    "# time_range = ['2020-01', '2020-02', '2020-03', '2020-04', '2020-05', '2020-06', '2020-07', '2020-08', '2020-09', '2020-10', '2020-11', '2020-12', \n",
    "#             '2021-01', '2021-02', '2021-03', '2021-04', '2021-05', '2021-06', '2021-07', '2021-08', '2021-09', '2021-10', '2021-11', '2021-12', \n",
    "#             '2022-01', '2022-02', '2022-03', '2022-04']\n",
    "# covidDataset = dr.COVIDDataRetriever().fetchAllTextFileURLs(time_range)\n",
    "\n",
    "# covidDataset[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd520ce4",
   "metadata": {},
   "source": [
    "### Save collection tweets into larger files to prepare for large data analysis. <a href name id=\"1a\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e514a8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves all tweets collecting on CS server into daily files\n",
    "\n",
    "basePath = '/students/jw10/cs315/collection-tweets/'\n",
    "writeToPath = '/students/jw10/cs315/tweets-by-day'\n",
    "dr.FileSynthesizer().combineHourlyTweetsToDaily(basePath, writeToPath, printOut=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a2ba017",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOTE: ALL LOCAL FILES HAVE BEEN ADDED TO THE `COLLECTION-LOGS` DIRECTORY. NO LONGER NEED TO RUN THIS\n",
    "# # Saves all tweets in local collection files into daily files\n",
    "# basePath = '/students/jw10/cs315/local-collection-tweets/'\n",
    "# writeToPath = '/students/jw10/cs315/tweets-by-day'\n",
    "# dr.FileSynthesizer().combineHourlyTweetsToDaily(basePath, writeToPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49ac77b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all tweets into one large file (uses the daily files we need to generate in the above cells)\n",
    "basePath='/students/jw10/cs315/tweets-by-day'\n",
    "writeToPath = '/students/jw10/cs315/all-tweets'\n",
    "\n",
    "dr.FileSynthesizer().getAllTweets(basePath, writeToPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c50622",
   "metadata": {},
   "source": [
    "### Open the .jsonl file containing all transportation tweets <a href name id=\"1b\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfaf9e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16936"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allTweets = []\n",
    "with open('/students/jw10/cs315/all-tweets/covid-mobility-tweet-all.jsonl') as f:\n",
    "    for line in f:\n",
    "        allTweets.append(json.loads(line))\n",
    "\n",
    "len(allTweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058564a7",
   "metadata": {},
   "source": [
    "### Observe some of the text data <a href name id=\"1c\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43a87b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnLoweredTweetText(tweet):\n",
    "    if \"retweeted_status\" not in tweet: \n",
    "        if 'full_text' not in tweet:\n",
    "            return tweet['text'].lower()\n",
    "        else:\n",
    "            return tweet['full_text'].lower()\n",
    "    else:\n",
    "        if 'full_text' not in tweet['retweeted_status']:\n",
    "            return tweet['retweeted_status']['text'].lower()\n",
    "        else:\n",
    "            return tweet['retweeted_status']['full_text'].lower()\n",
    "\n",
    "def printTweetText(tweet):\n",
    "    print('*** Tweet ***')\n",
    "    if \"retweeted_status\" not in tweet: \n",
    "        if 'full_text' not in tweet:\n",
    "            print(tweet['text'])\n",
    "        else:\n",
    "            print(tweet['full_text'])\n",
    "    else:\n",
    "        if 'full_text' not in tweet['retweeted_status']:\n",
    "            print(tweet['retweeted_status']['text'])\n",
    "        else:\n",
    "            print(tweet['retweeted_status']['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f044892f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Tweet ***\n",
      "This is what #Thanksgiving traffic looked like in Los Angeles before COVID-19 https://t.co/IS91onrhRU\n",
      "--------------------------------------------------------------------------------\n",
      "*** Tweet ***\n",
      "@TimSmithMP 690+ of which were in Federal (LNP) Government run aged care facilities. ZERO covid deaths in state run aged car facilities in Vic.\n",
      "--------------------------------------------------------------------------------\n",
      "*** Tweet ***\n",
      "Closing the station because everyone has COVID and then blaming it on ‚Äúdefunding‚Äù that 100% has not happened is the most cop shit ever. They know their supporters will run will a false narrative, even if they know it‚Äôs clearly false. Incredible. https://t.co/33hUqAH0Cj\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "random.seed(2) # I'm doing this so that the sequence of random numbers is replicable\n",
    "\n",
    "for i in range(3):\n",
    "    tweet = random.choice(allTweets)\n",
    "    printTweetText(tweet)\n",
    "    print(80*'-')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb35d24",
   "metadata": {},
   "source": [
    "We notice that a lot of the tweets are cut off. This was an observation I made in the last milestone when I found out that you must add a parameter called `extend_mode` to the tweepy `getStatus` request. We will dehydrate our transportation files into their tweet IDs and then rehydrate them to see if we get the full text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40a40f1",
   "metadata": {},
   "source": [
    "### Dehydrate and rehydrate tweets to obtain `full_text` field. <a href name id=\"1d\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81262ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dehydrate full list of tweets .jsonl file\n",
    "os.system('twarc dehydrate /students/jw10/cs315/all-tweets/covid-mobility-tweet-all.jsonl > /students/jw10/cs315/all-tweets/covid-mobility-tweet-all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d00df70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rehydrate the full list of tweets from .txt. to .jsonl file to get full text.\n",
    "\n",
    "os.system('twarc hydrate /students/jw10/cs315/all-tweets/covid-mobility-tweet-all.txt > /students/jw10/cs315/all-tweets/covid-mobility-tweet-all-rehydrated.jsonl' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e3ca9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16789"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allTweets = []\n",
    "with open('/students/jw10/cs315/all-tweets/covid-mobility-tweet-all-rehydrated.jsonl') as f:\n",
    "    for line in f:\n",
    "        allTweets.append(json.loads(line))\n",
    "\n",
    "len(allTweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccb026f",
   "metadata": {},
   "source": [
    "**Observations**: It appears that we have lost ~30 tweets that were in our initial `covid-mobility-tweet-all.jsonl` file. It is possible that these were repeat tweets which `twarc` removed on its own or that they were incorrectly formatted. For now, we will ignore this because it does not make a large impact on the size of our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3f7032",
   "metadata": {},
   "source": [
    "### Observe tweet text after rehydrating <a href name id=\"1e\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e89925cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Tweet ***\n",
      "COVID Update July 22: The ‚Äúanti‚Äù culture in America is a long run hazard to public health &amp; a near term hazard to yours.\n",
      "\n",
      "We need to address it &amp; turn it around.  1/\n",
      "--------------------------------------------------------------------------------\n",
      "*** Tweet ***\n",
      "Yes @JustinTrudeau , take the #AstraZeneca #vaccine ! If you take it, I‚Äôll take it also. Now it‚Äôs the time to walk the talk! #cdnpoli #GetVaccinated #Covid19 #Canada #Ottawa #ableg #abpoli #onpoli #bcpoli #Yeg #yyc #COVID19Ontario #LPC @gmbutts #VaccineForAll #Toronto #Ontario https://t.co/kKcTQ60g3m\n",
      "--------------------------------------------------------------------------------\n",
      "*** Tweet ***\n",
      "#COVID19 | In Cork on Sunday, garda√≠ broke up a score of road bowling on roads in the Bottlehill area of Burnfort, just south of Mallow, with up to 50 people in attendance reports @EoinBearla\n",
      "https://t.co/3oQtthZlP4\n",
      "--------------------------------------------------------------------------------\n",
      "*** Tweet ***\n",
      "New Covid Cases Expected To Level Off As Trend Line Reaches Top Of Graph  https://t.co/2EwlPmgZM1 https://t.co/A2R3CQ3YGG\n",
      "--------------------------------------------------------------------------------\n",
      "*** Tweet ***\n",
      "@Scotpol1314 @SNPsoosie Agree that time is running out fast ! Clear path to Indy must be laid down in the next couple of months. WM is going to be in the shite in new year with Brexit and COVID so that‚Äôs the time to make a move.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "random.seed(2) # I'm doing this so that the sequence of random numbers is replicable\n",
    "\n",
    "for i in range(5):\n",
    "    tweet = random.choice(allTweets)\n",
    "    printTweetText(tweet)\n",
    "    print(80*'-')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ceff6d",
   "metadata": {},
   "source": [
    "## Creating a tweet classifier <a href name id=\"2\">\n",
    "\n",
    "Some of these tweets are unrelated to transportation. We build a simple classifier to test whether or not these tweets are relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24a71ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import inflect\n",
    "\n",
    "p = inflect.engine()\n",
    "\n",
    "def saveKeywords():\n",
    "    keywords = pd.read_csv('./cs315_keywords.csv')\n",
    "    # lowercase all keyword strings\n",
    "    keywords['public transport'] = keywords['public transport'].str.lower()\n",
    "    keywords['motorized'] = keywords['motorized'].str.lower()\n",
    "    keywords['non-motorized'] = keywords['non-motorized'].str.lower()\n",
    "\n",
    "    transit = [word for word in keywords['public transport'] if not pd.isna(word)]\n",
    "    motorized = [word for word in keywords['motorized'] if not pd.isna(word)]\n",
    "    nonmotorized = [word for word in keywords['non-motorized'] if not pd.isna(word)]\n",
    "\n",
    "    pluralKeywords = [p.plural(word) for word in (transit + motorized + nonmotorized)] # not exhaustive, but captures some meaning\n",
    "\n",
    "    allKeywords = set(transit + motorized + nonmotorized + pluralKeywords)\n",
    "    return allKeywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc3005c",
   "metadata": {},
   "source": [
    "First we use the `inflect` library to pluralize all our keywords. Some words were already in plural form from when I manualled coded them, but I wish I knew about this library earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5501c4",
   "metadata": {},
   "source": [
    "### Classifier to remove Words With Multiple Meanings <a href name id=\"2a\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c541024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweetClassifier1(tweet, keywords):\n",
    "    '''\n",
    "    This classifier captures a list of tweets that might be unrelated to transportation. It uses the following metrics:\n",
    "    \n",
    "    If tweet contains \"line\", \"run\", \"running\", or \"ford\" it checks if another transportation keyword is also in the tweet\n",
    "    \n",
    "    Parameters:\n",
    "    tweet - the tweet containing a text body to be parsed\n",
    "    keywords - all transportation keywords\n",
    "    '''\n",
    "    uncertainKeywords = {'line', 'run', 'running', 'ford', 'lincoln'}\n",
    "    tweetText = returnLoweredTweetText(tweet)\n",
    "    tweetTextSet = set(tweetText.split())\n",
    "\n",
    "    if uncertainKeywords.intersection(tweetTextSet):\n",
    "        remainingKeywords = keywords.difference(uncertainKeywords)\n",
    "        if remainingKeywords.intersection(tweetTextSet):\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "    \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b27d30db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Irrelevant: 6441 Relevant: 10348\n"
     ]
    }
   ],
   "source": [
    "# Run tweet classifier 1\n",
    "transportationKeywords = saveKeywords()\n",
    "irrelevantTweets = []\n",
    "relevantTweets = []\n",
    "\n",
    "for tweet in allTweets: \n",
    "    classification = tweetClassifier1(tweet, transportationKeywords)\n",
    "    if classification == -1:\n",
    "        irrelevantTweets.append(tweet)\n",
    "#         printTweetText(tweet)\n",
    "#         print(80*'-')  \n",
    "    else:\n",
    "        relevantTweets.append(tweet)\n",
    "        \n",
    "print(\"Irrelevant:\",len(irrelevantTweets), \"Relevant:\",len(relevantTweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80443374",
   "metadata": {},
   "source": [
    "### Observe some irrelevant tweets to see if we're capturing accurate information <a href name id=\"2b\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7782094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Tweet ***\n",
      "VP Leni's focus remains on the Covid-19 crisis. She has not decided on 2022, and is not \"preparing\" to run for Governor. She remains open to all options, including a candidacy for President, and at the appropriate time, she will personally convey her decision on this matter.\n",
      "--------------------------------------------------------------------------------\n",
      "*** Tweet ***\n",
      "Centre withdraws insurance cover for healthcare workers who succumbed in the line of #Covid duty\n",
      "‚óè Collapsed healthcare system &amp; now deceit with Safai karamcharis, ward-boys, nurses, ASHA workers, paramedics, technicians, doctors! #ModiResign \n",
      "\n",
      "https://t.co/2NoiWqixKi\n",
      "--------------------------------------------------------------------------------\n",
      "*** Tweet ***\n",
      "Cuomo is cutting Medicaid mid-pandemic. He should never get to run for office as a Democrat again. https://t.co/ABc3z7dulR\n",
      "--------------------------------------------------------------------------------\n",
      "*** Tweet ***\n",
      "Medical supplies continue to arrive at Wuhan Tianhe International Airport as local hospitals are running short of medical resources facing the novel #coronavirus outbreak https://t.co/cbCTOeuo4g\n",
      "--------------------------------------------------------------------------------\n",
      "*** Tweet ***\n",
      "I‚Äôm coming back this this bc during the Olympics in ATL they were planning on running the torch through Greenville but the city was in the middle of some homophobic shit that the Olympic committee was basically either get it together or we don‚Äôt come there https://t.co/BffPDby9zX\n",
      "--------------------------------------------------------------------------------\n",
      "*** Tweet ***\n",
      "Let's run a Blue Checkmark thought experiment. Same Constitutional amendment, but imagine that instead of \"synagogues\" he said \"newsrooms.\" https://t.co/kmL65KGL0s\n",
      "--------------------------------------------------------------------------------\n",
      "*** Tweet ***\n",
      "My fellow unjabbed Australian's ‚ô•Ô∏è\n",
      "Now that the Australian Bureau of Statistics has released its Covid stats...\n",
      "How good is it to know that you HELD THE LINE üòò\n",
      "--------------------------------------------------------------------------------\n",
      "*** Tweet ***\n",
      "To counter all the Mango Mendaciousness, sometimes you run across something beautiful like this &gt;&gt;&gt;&gt;&gt; https://t.co/4I9ntvxxO7\n",
      "--------------------------------------------------------------------------------\n",
      "*** Tweet ***\n",
      "@TaprimaryU @jacquep Your first duty is to protect citizens @sajidjavid @10DowningStreet @nadhimzahawi #COVID19 is running riot in schools again! @grantshapps by removing PLFs you open the door to infection breakouts without any indicators where the infected people are! @CMO_England @uksciencechief ?\n",
      "--------------------------------------------------------------------------------\n",
      "*** Tweet ***\n",
      "@christwattering @JH_Campbell @AJS77 @JVSwallow There were 4 toilets for 10,000 trucks - no hot running water and the only food being handed out by the govt were cereal bars. We've been living with this pandemic for 10 mths. We've watched the same happen in Europe &amp; still we were unprepared its embarrassing.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(5) # I'm doing this so that the sequence of random numbers is replicable\n",
    "\n",
    "for i in range(10):\n",
    "    tweet = random.choice(irrelevantTweets)\n",
    "    printTweetText(tweet)\n",
    "    print(80*'-')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9146414",
   "metadata": {},
   "source": [
    "As you can see, many of the tweets marked as `irrelevant` do **not** contain information related to transportation. Our classifier found that about 1/3 of our collected data is irrelevant! We create a function that captures only relevant tweet IDs and store them in a `.txt` file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b7efcd",
   "metadata": {},
   "source": [
    "### Create a classifier which removes non-English tweets <a href name id=\"2c\">\n",
    "    \n",
    "We will utilize both Twitter's 'lang' field and the library `langid` to determine whether tweets are English or not. We remove non-English tweets from our corpus.\n",
    "    \n",
    "Update: we realize that all the tweets contain the `lang` field and thus can be analyzed without using a heavy library. Thus, we stick to our current classifier using just the set of languages that Twitter supports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40cd20e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ja', 'fil', 'en-gb', 'de', 'th', 'ru', 'pl', 'es', 'msa', 'he', 'nl', 'hi', 'sv', 'da', 'fa', 'en', 'zh-tw', 'zh-cn', 'fr', 'fi', 'pt', 'ur', 'ar', 'ko', 'no', 'it', 'hu', 'tr', 'id'}\n"
     ]
    }
   ],
   "source": [
    "# language codes from: https://developer.twitter.com/en/docs/twitter-api/v1/developer-utilities/supported-languages/api-reference/get-help-languages\n",
    "codes = [\n",
    "  {\n",
    "    \"code\": \"fr\",\n",
    "    \"status\": \"production\",\n",
    "    \"name\": \"French\"\n",
    "  },\n",
    "  {\n",
    "    \"code\": \"en\",\n",
    "    \"status\": \"production\",\n",
    "    \"name\": \"English\"\n",
    "  },\n",
    "  {\n",
    "    \"code\": \"ar\",\n",
    "    \"status\": \"production\",\n",
    "    \"name\": \"Arabic\"\n",
    "  },\n",
    "  {\n",
    "    \"code\": \"ja\",\n",
    "    \"status\": \"production\",\n",
    "    \"name\": \"Japanese\"\n",
    "  },\n",
    "  {\n",
    "    \"code\": \"es\",\n",
    "    \"status\": \"production\",\n",
    "    \"name\": \"Spanish\"\n",
    "  },\n",
    "  {\n",
    "    \"code\": \"de\",\n",
    "    \"status\": \"production\",\n",
    "    \"name\": \"German\"\n",
    "  },\n",
    "  {\n",
    "    \"code\": \"it\",\n",
    "    \"status\": \"production\",\n",
    "    \"name\": \"Italian\"\n",
    "  },\n",
    "  {\n",
    "    \"code\": \"id\",\n",
    "    \"status\": \"production\",\n",
    "    \"name\": \"Indonesian\"\n",
    "  },\n",
    "  {\n",
    "    \"code\": \"pt\",\n",
    "    \"status\": \"production\",\n",
    "    \"name\": \"Portuguese\"\n",
    "  },\n",
    "  {\n",
    "    \"code\": \"ko\",\n",
    "    \"status\": \"production\",\n",
    "    \"name\": \"Korean\"\n",
    "  },\n",
    "  {\n",
    "    \"code\": \"tr\",\n",
    "    \"status\": \"production\",\n",
    "    \"name\": \"Turkish\"\n",
    "  },\n",
    "  {\n",
    "    \"code\": \"ru\",\n",
    "    \"status\": \"production\",\n",
    "    \"name\": \"Russian\"\n",
    "  },\n",
    "  {\n",
    "    \"code\": \"nl\",\n",
    "    \"status\": \"production\",\n",
    "    \"name\": \"Dutch\"\n",
    "  },\n",
    "  {\n",
    "    \"code\": \"fil\",\n",
    "    \"status\": \"production\",\n",
    "    \"name\": \"Filipino\"\n",
    "  },\n",
    "  {\n",
    "    \"code\": \"msa\",\n",
    "    \"status\": \"production\",\n",
    "    \"name\": \"Malay\"\n",
    "  },\n",
    "  {\n",
    "    \"code\": \"zh-tw\",\n",
    "    \"status\": \"production\",\n",
    "    \"name\": \"Traditional Chinese\"\n",
    "  },\n",
    "  {\n",
    "    \"code\": \"zh-cn\",\n",
    "    \"status\": \"production\",\n",
    "    \"name\": \"Simplified Chinese\"\n",
    "  },\n",
    "  {\n",
    "    \"code\": \"hi\",\n",
    "    \"status\": \"production\",\n",
    "    \"name\": \"Hindi\"\n",
    "  },\n",
    "  {\n",
    "    \"code\": \"no\",\n",
    "    \"status\": \"production\",\n",
    "    \"name\": \"Norwegian\"\n",
    "  },\n",
    "  {\n",
    "    \"code\": \"sv\",\n",
    "    \"status\": \"production\",\n",
    "    \"name\": \"Swedish\"\n",
    "  },\n",
    "  {\n",
    "    \"code\": \"fi\",\n",
    "    \"status\": \"production\",\n",
    "    \"name\": \"Finnish\"\n",
    "  },\n",
    "  {\n",
    "    \"code\": \"da\",\n",
    "    \"status\": \"production\",\n",
    "    \"name\": \"Danish\"\n",
    "  },\n",
    "  {\n",
    "    \"code\": \"pl\",\n",
    "    \"status\": \"production\",\n",
    "    \"name\": \"Polish\"\n",
    "  },\n",
    "  {\n",
    "    \"code\": \"hu\",\n",
    "    \"status\": \"production\",\n",
    "    \"name\": \"Hungarian\"\n",
    "  },\n",
    "  {\n",
    "    \"code\": \"fa\",\n",
    "    \"status\": \"production\",\n",
    "    \"name\": \"Farsi\"\n",
    "  },\n",
    "  {\n",
    "    \"code\": \"he\",\n",
    "    \"status\": \"production\",\n",
    "    \"name\": \"Hebrew\"\n",
    "  },\n",
    "  {\n",
    "    \"code\": \"ur\",\n",
    "    \"status\": \"production\",\n",
    "    \"name\": \"Urdu\"\n",
    "  },\n",
    "  {\n",
    "    \"code\": \"th\",\n",
    "    \"status\": \"production\",\n",
    "    \"name\": \"Thai\"\n",
    "  },\n",
    "  {\n",
    "    \"code\": \"en-gb\",\n",
    "    \"status\": \"production\",\n",
    "    \"name\": \"English UK\"\n",
    "  }\n",
    "]\n",
    "\n",
    "languages = set([item['code'] for item in codes])\n",
    "\n",
    "print(languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38300c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n"
     ]
    }
   ],
   "source": [
    "# Tests langid\n",
    "import langid\n",
    "\n",
    "s = \"zhe shi zhong wen\"\n",
    "s2 = \"I can't believe they would do something like that\"\n",
    "s3 = \"I‚Äôll never understand how folks can hear ‚ÄòPa is running out of ICU beds in a pandemic‚Äô and two seconds later flip out because they‚Äôre closing bars for one ‚Äî one ‚Äî night.  At a complete loss of how these folks process the world.\"\n",
    "lang, confidence = langid.classify(s3)\n",
    "print(lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9c5780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweetClassifier2(tweet):\n",
    "    '''\n",
    "    Classifies English tweets as 1 and non-English tweets as -1\n",
    "    '''\n",
    "\n",
    "    if 'lang' in tweet:\n",
    "        if tweet['lang'] not in {'en-gb', 'en'}:\n",
    "            return -1\n",
    "        else:\n",
    "            if tweet['lang'] != \"en\":\n",
    "                print(\"this is in the field: \",tweet['lang'])\n",
    "            return 1\n",
    "    else:\n",
    "        print('no language field')\n",
    "        tweetText = returnLoweredTweetText(tweet)\n",
    "        lang, confidence = langid.classify(tweetText)\n",
    "        if lang.equals('en'):\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "        \n",
    "def tweetClassifier_withlibrary(tweet):\n",
    "    '''\n",
    "    Classifies English tweets as 1 and non-English tweets as -1\n",
    "    '''\n",
    "    \n",
    "    tweetText = returnLoweredTweetText(tweet)\n",
    "    lang, confidence = langid.classify(tweetText)\n",
    "    if lang == 'en':\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c1a3ae",
   "metadata": {},
   "source": [
    "## Store relevant tweet IDs in `.txt` file <a href name id=\"3a\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "553fccd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterRelevantTweetIDs(allTweets, keywords, languages):\n",
    "    '''\n",
    "    Uses the classifier to remove irrelevant tweets. Stores relevant tweets in a list.\n",
    "    '''\n",
    "    relevantTweets = []\n",
    "    for tweet in allTweets: \n",
    "        classification1 = tweetClassifier1(tweet, transportationKeywords)\n",
    "        classification2 = tweetClassifier2(tweet)\n",
    "        if classification1 + classification2 == 2:\n",
    "            relevantTweets.append(tweet['id'])\n",
    "    \n",
    "    return relevantTweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a64b62ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9125\n"
     ]
    }
   ],
   "source": [
    "transportationKeywords = saveKeywords()\n",
    "relevantTweets = filterRelevantTweetIDs(allTweets, transportationKeywords, languages)\n",
    "print(len(relevantTweets))\n",
    "with open('/students/jw10/cs315/all-tweets/covid-mobility-tweet-all-relevant.txt', 'w') as f:\n",
    "    for tweetID in relevantTweets:\n",
    "        f.write(f'{tweetID} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e209507",
   "metadata": {},
   "source": [
    "### Rehydrate our relevant tweets <a href name id=\"3a\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "269179f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('twarc hydrate /students/jw10/cs315/all-tweets/covid-mobility-tweet-all-relevant.txt > /students/jw10/cs315/all-tweets/covid-mobility-tweet-all-relevant.jsonl' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af048a10",
   "metadata": {},
   "source": [
    "### Now we're ready to do some analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf019a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
