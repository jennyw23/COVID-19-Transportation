{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81fd61bf",
   "metadata": {},
   "source": [
    "# Part 1: Data Collection\n",
    "\n",
    "### Overview:\n",
    "This notebook creates basic functions to parse the COVID-19 GitHub repo, get Tweet IDs from a text file, open tweet JSON from a Tweet ID, etc. It also does initial COVID-19+Transportation tasks by parsing our keywords data set and defining functions to fetch COVID-19+Transportation tweets. Lastly, these functions help make up the `general-collector.py` script which we use to scrape most of our data from the COVID-19 repo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09c45b4",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "1. [Parse COVID-19 Dataset](#1)\n",
    "    1. [Access the COVID-19 dataset files](#a)\n",
    "    2. [Get Tweet IDs from a text file](#b)\n",
    "2. [Parse Tweet IDs for text body](#2)\n",
    "    1. [Set up Twitter API keys and initialize necessary libraries](#2a)\n",
    "    2. ['Transform' keywords csv file and define functions for keyword searching](#2b)\n",
    "    3. [Save the date range of the transportation tweets saved in each file](#2c)\n",
    "    3. [Define functions to fetch and save COVID-19+Transportation data](#2d)\n",
    "3. [Finally, call the Twitter API](#3) \n",
    "4. [Dehydrate `jsonl` files back to `txt` file of tweet IDs](#4)\n",
    "5. [Update: Data Collection through `general-collector.py` (4/7/2022)](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88b989e",
   "metadata": {},
   "source": [
    "### My data collection involves a three-step process:\n",
    "1. Access the tweet ID text files from the [COVID-19 dataset](https://github.com/echen102/COVID-19-TweetIDs).\n",
    "2. Combine tweet ID text files into larger documents to \"rehydrate\" the tweet IDs into their full json form.\n",
    "3. Filter COVID tweets by keywords related to transportation.\n",
    "\n",
    "\n",
    "Goal: create training dataset. No corresponding labels (this is unsupervised)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e63fc04",
   "metadata": {},
   "source": [
    "## (1) Parse COVID-19 Dataset <a href name id=\"1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bc3393",
   "metadata": {},
   "source": [
    "#### Key Events\n",
    "Using the [CDC COVID-19 Timeline](https://www.cdc.gov/museum/timeline/covid19.html), we focus our data collection on key time periods in which tweets related to COVID-19 intersect with phrases related to transportation.\n",
    "\n",
    "Origins: December 2019 - January 2020\n",
    "- December 12, 2019: Patients in Wuhan begin experiencing symptoms of what later becomes known as COVID-19.\n",
    "- January 20, 2020: CDC confirms the first U.S. laboratory-confirmed case of COVID-19 in the U.S. from samples taken on January 18 in Washington state.\n",
    "\n",
    "Early Pandemic: February 2020 - May 2020\n",
    "- February 23, 2020: Italy becomes a global COVID-19 hotspot. \n",
    "- [February 26, 2020](https://www.cdc.gov/media/releases/2020/t0225-cdc-telebriefing-covid-19.html): CDC’s Dr. Nancy Messonnier, Incident Manager for the COVID-19 Response, holds a telebriefing. During the telebriefing she braces the U.S. for the eventual community spread of the novel coronavirus and states that the “disruption to everyday life may be severe.”\n",
    "- March 11, 2020: The World Health Organization declares COVID-19 a pandemic.\n",
    "- March 13, 2020: President Donald J. Trump declares a nationwide emergency.\n",
    "- March 15, 2020: U.S. states begin to shut down to prevent the spread of COVID-19. New York City public schools system (the largest school system in the U.S., with 1.1 million students) shuts down, while Ohio calls for restaurants and bars to close.\n",
    "- March 28, 2020: White House extends social distancing measures until the end of April 2020.\n",
    "- April 3, 2020: At a White House press briefing, CDC announces new mask wearing guidelines and recommends that all people wear a mask when outside of the home.\n",
    "- May 2, 2020: World Health Organization renews its emergency declaration from three months prior calling the pandemic a global health crisis.\n",
    "- May 8, 2020: News media outlets report that top White House officials shelve CDC “Guidance for Implementing the Opening Up America Again Framework” that include detailed advice on how to safely reopen the country.\n",
    "- July 23, 2020: CDC releases new science-based resources and tolls for school administrators, teachers, parents, guardians, and caregivers for safe school reopening.\n",
    "\n",
    "COVID Vaccines Rollout: December 2020 - April 2021\n",
    "- December 11, 2020: Food and Drug Administration issues an Emergency Use Authorization (EUA) for the first COVID-19 vaccine – the Pfizer-BioNTech COVID-19 vaccine.\n",
    "- December 24, 2020: It is estimated that more than 1 million people in the U.S. are vaccinated against COVID-19.\n",
    "- March 8, 2021: CDC announces that fully vaccinated people can gather indoors without masks.\n",
    "- April 2, 2021: CDC announces fully vaccinated individuals can travel safely domestically in the U.S. without a COVID test first.\n",
    "- July 27, 2021: After a substantial upswing in cases due to the Delta variant, CDC releases updated guidance for everyone in areas with substantial or high transmission to wear a mask while indoors.\n",
    "- November 29, 2021: CDC recommends that everyone over 18 years old who received a Pfizer or Moderna vaccine receive a COVID-19 booster shot 6 months after they are fully vaccinated.\n",
    "\n",
    "- **March 10, 2022: At CDC’s recommendation, TSA extends the security directive for mask use on public transportation and transportation hubs for one month, through April 18th.**\n",
    "\n",
    "\n",
    "\n",
    "https://www.cdc.gov/media/releases/2020/t0225-cdc-telebriefing-covid-19.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e430e74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "# twitter API-related\n",
    "import tweepy\n",
    "from tqdm import tqdm\n",
    "from twarc import Twarc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ae4300",
   "metadata": {},
   "source": [
    "### Access the COVID-19 dataset files <a href name id=\"a\">\n",
    "According to the COVID-19 dataset documentation, the Tweet-IDs are organized as follows:\n",
    "- Tweet-ID files are stored in folders that indicate the year and month of the collection (YEAR-MONTH).\n",
    "- Individual Tweet-ID files contain a collection of Tweet IDs, and the file names all follow the same structure, with a prefix “coronavirus-tweet-id-” followed by the YEAR-MONTH-DATE-HOUR.\n",
    "\n",
    "The COVID-19 Tweet IDs are uploaded from 0:00-23:00, representing each hour in the day. Since some files may be missing (several hours in the day did not upload), we iterate over the files using the `YEAR-MONTH-DATE-HOUR` pattern, ignoring any URLs in which there is a `404 Not Found` Error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9974058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://github.com/echen102/COVID-19-TweetIDs/tree/master/2020-01\n",
      "https://github.com/echen102/COVID-19-TweetIDs/tree/master/2020-02\n",
      "https://github.com/echen102/COVID-19-TweetIDs/tree/master/2020-03\n",
      "https://github.com/echen102/COVID-19-TweetIDs/tree/master/2020-04\n",
      "https://github.com/echen102/COVID-19-TweetIDs/tree/master/2020-05\n",
      "https://github.com/echen102/COVID-19-TweetIDs/tree/master/2020-06\n",
      "https://github.com/echen102/COVID-19-TweetIDs/tree/master/2020-07\n",
      "https://github.com/echen102/COVID-19-TweetIDs/tree/master/2020-08\n",
      "https://github.com/echen102/COVID-19-TweetIDs/tree/master/2020-09\n",
      "https://github.com/echen102/COVID-19-TweetIDs/tree/master/2020-10\n",
      "https://github.com/echen102/COVID-19-TweetIDs/tree/master/2020-11\n",
      "https://github.com/echen102/COVID-19-TweetIDs/tree/master/2020-12\n",
      "https://github.com/echen102/COVID-19-TweetIDs/tree/master/2021-01\n",
      "https://github.com/echen102/COVID-19-TweetIDs/tree/master/2021-02\n",
      "https://github.com/echen102/COVID-19-TweetIDs/tree/master/2021-03\n",
      "https://github.com/echen102/COVID-19-TweetIDs/tree/master/2021-04\n",
      "https://github.com/echen102/COVID-19-TweetIDs/tree/master/2021-05\n",
      "https://github.com/echen102/COVID-19-TweetIDs/tree/master/2021-06\n",
      "https://github.com/echen102/COVID-19-TweetIDs/tree/master/2021-07\n",
      "https://github.com/echen102/COVID-19-TweetIDs/tree/master/2021-08\n",
      "https://github.com/echen102/COVID-19-TweetIDs/tree/master/2021-09\n",
      "https://github.com/echen102/COVID-19-TweetIDs/tree/master/2021-10\n",
      "https://github.com/echen102/COVID-19-TweetIDs/tree/master/2021-11\n",
      "https://github.com/echen102/COVID-19-TweetIDs/tree/master/2021-12\n",
      "https://github.com/echen102/COVID-19-TweetIDs/tree/master/2022-01\n",
      "https://github.com/echen102/COVID-19-TweetIDs/tree/master/2022-02\n",
      "https://github.com/echen102/COVID-19-TweetIDs/tree/master/2022-03\n",
      "https://github.com/echen102/COVID-19-TweetIDs/tree/master/2022-04\n"
     ]
    }
   ],
   "source": [
    "# get outer directories\n",
    "base_url = 'https://github.com/echen102/COVID-19-TweetIDs/tree/master/'\n",
    "time_range = ['2020-01', '2020-02', '2020-03', '2020-04', '2020-05', '2020-06', '2020-07', '2020-08', '2020-09', '2020-10', '2020-11', '2020-12', \n",
    "            '2021-01', '2021-02', '2021-03', '2021-04', '2021-05', '2021-06', '2021-07', '2021-08', '2021-09', '2021-10', '2021-11', '2021-12', \n",
    "            '2022-01', '2022-02', '2022-03', '2022-04']\n",
    "\n",
    "# show the Tweet-ID `YEAR-MONTH` folders\n",
    "for yyyyMM in time_range:\n",
    "    print(base_url+yyyyMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b25c452e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files in Time Range: 20832\n"
     ]
    }
   ],
   "source": [
    "# selected_time_range = ['2020-01', '2020-02']\n",
    "\n",
    "def fetchAllTextFileURLs(time_range):\n",
    "    files = []\n",
    "    base_url = 'https://raw.githubusercontent.com/echen102/COVID-19-TweetIDs/master/'\n",
    "    prefix = 'coronavirus-tweet-id'\n",
    "    dates = list(range(1, 32)) # day 1 to 31\n",
    "    hours = list(range(24)) # hour 0 to 23\n",
    "    \n",
    "    for yyyyMM in time_range:\n",
    "        for date in dates:\n",
    "            date = f'0{date}' if date < 10 else f'{date}'\n",
    "            for hour in hours:\n",
    "                hour = f'0{hour}' if hour < 10 else f'{hour}'\n",
    "                fileURL = f'{base_url}{yyyyMM}/{prefix}-{yyyyMM}-{date}-{hour}.txt'\n",
    "                files.append(fileURL)\n",
    "\n",
    "                # print(fileURL)\n",
    "    \n",
    "    return files\n",
    "\n",
    "# testFiles = fetchAllTextFileURLs(selected_time_range)\n",
    "testFiles = fetchAllTextFileURLs(time_range)\n",
    "\n",
    "print('Total files in Time Range:', len(testFiles))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05caa5e8",
   "metadata": {},
   "source": [
    "### Get Tweet IDs from a text file. <a href name id=\"b\">\n",
    "The algorithm we used to generate the dataset files used the dataset's labels to will generate all dates/times between January 1, 2020 and April, 2022 if we set the time range as 1/2020 to 4/2022. Since the COVID-19's data collection did not begin until January 21 and known data collection gaps have occurred from then to now, we cannot rely on the URL being 100% valid. \n",
    "\n",
    "Thus, for each url generated, we call `getTweetIDsFrom(txtFile)` to check if the file exists. If it has a `404 File Not Found` error, then we ignore it. Otherwise, we run through the `.txt` file to retrieve all the tweet IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f16cec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTweetIDsFrom(txtFile):\n",
    "    '''\n",
    "    Retrieves tweet IDs from the text file's URL and stores in list\n",
    "    '''\n",
    "    tweetList = []\n",
    "    response = requests.get(txtFile) # requests raw file from Github\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        content = response.content.decode('utf-8')\n",
    "        tweetList = content.split()\n",
    "        \n",
    "    return tweetList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e17bf1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://raw.githubusercontent.com/echen102/COVID-19-TweetIDs/master/2020-01/coronavirus-tweet-id-2020-01-23-07.txt\n",
      "https://raw.githubusercontent.com/echen102/COVID-19-TweetIDs/master/2020-02/coronavirus-tweet-id-2020-02-11-16.txt\n",
      "January 23, 2020, at 7am : 2006\n",
      "February 11, 2021, at 4pm: 41281\n"
     ]
    }
   ],
   "source": [
    "# sample URLS for testing\n",
    "url2020 = testFiles[535]\n",
    "url2021 = testFiles[1000]\n",
    "\n",
    "print(url2020)\n",
    "tweets2020 = getTweetIDsFrom(url2020)\n",
    "print(url2021)\n",
    "tweets2021 = getTweetIDsFrom(url2021)\n",
    "\n",
    "print(\"January 23, 2020, at 7am :\",len(tweets2020))\n",
    "print(\"February 11, 2021, at 4pm:\", len(tweets2021))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b960fc",
   "metadata": {},
   "source": [
    "From our two tests, we see that the volume of tweets increased significantly as recognition of the COVID-19 pandemic spread. In late January 2020, we see that there are just ~2000 hourly tweets about COVID-19, while in mid February 2021, there are about ~41,000 hourly tweets.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182a60d9",
   "metadata": {},
   "source": [
    "## (2) Parse Tweet IDs for text body. <a href name id='2'>\n",
    "\n",
    "We are now ready to begin our search for transportation data within the COVID-19 dataset. We created a \"keywords\" list of possible phrases and words related to transportation and mobility. The list is not exhaustive, but seeks to include phrases that are not directly related to my analysis to ensure we have as robust a dataset as possible.\n",
    "\n",
    "`Twarc` is a Python library that collects and archives Twitter data via the Twitter API. The benefit of `Twarc` is that it contains two methods, called `hydrate` and `dehydrate` which can generate tweet JSON from a `.txt` file of tweet IDs and generate tweet IDs from a file of tweets.\n",
    "\n",
    "In our analysis, we will first generate json files of `COVID-19+transportation` tweets by iterating through the COVID-19 dataset. After generating json files capped ~1000 tweets, we will use Twarc to `dehydrate` the tweets into a compact `.txt` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8327fff7",
   "metadata": {},
   "source": [
    "### Set up Twitter API keys and initialize necessary libraries <a href name id=\"2a\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f817a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ****Set up to access Twitter API****\n",
    "\n",
    "# Assign developer keys\n",
    "consumer_key='Hdzqjxf3mVyw5DQLUISQO8Dsz'\n",
    "consumer_secret='9NN8pQz1gKfk8wIy8VnPXM2TWtvmvnW3n19rAavuo5MGiirDey'\n",
    "access_token='1345900794625847301-pPqmLUdewBlbRx5awibCmvajKh6qnK'\n",
    "access_token_secret='bVVGLYrMxqssj1RkEUigABAd5mBZ9i4RL1nP6j6tbeDmm'\n",
    "  \n",
    "# authorization of consumer key and consumer secret\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "  \n",
    "# set access to user's access key and access secret \n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "  \n",
    "# calling the api \n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1683af",
   "metadata": {},
   "source": [
    "### 'Transform' keywords csv file and define functions for keyword searching <a href name id='2b'> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f235201a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{nan, 'red line', 'buick', 'bike', 'chevrolet', 'monorail', 'autos', 'mass transit', 'rail', 'walks', 'automobiles', 'light rail', 'traffic', 'chrysler', 'jog', 'ford', 'muni', 'road', 'electric scooter', 'fiat', 'nissan', 'railroads', 'streetcars', 'mazda', 'vehicles', 'gasoline', 'silver line', 'cars', 'railway', 'scooters', 'kia', 'bart', 'bicycling', 'caltrain', 'jeep', 'lexus', 'rollerskate', 'commuter', 'route', 'rideshare', 'run', 'rollerblading', 'mta', 'roads', 'transportation', 'uber', 'metropolitan', 'hyundai', 'streetcar', 'railroad', 'commute', 'bus', 'walking', 'shuttle', 'amtrak', 'e-scooter', 'auto', 'bicycles', 'diesel', 'cable car', 'toyota', 'jogging', 'blue line', 'station', 'mass transportation', 'congestion', 'rides', 'honda', 'trolley', 'bicycle', 'engine', 'lincoln', 'commuter rail', 'highway', 'subaru', 'roadway', 'gas prices', 'lane', 'carpooling', 'workout', 'carpool', 'green line', 'bikers', 'walkers', 'running', 'exercise', 'transport', 'subway', 'rapid transit', 'metrorail', 'buses', 'pedestrian', 'dodge', 'line', 'mbta', 'rollerblade', 'cycle', 'car', 'acura', 'mitsubishi', 'cycling', 'scooter', 'cycles', 'transit', 'vehicle', 'lyft', 'walk', 'automobile', 'heavy rail', 'metro', 'infiniti', 'freeway'}\n",
      "112\n"
     ]
    }
   ],
   "source": [
    "# Transform keywords list\n",
    "\n",
    "keywords = pd.read_csv('./cs315_keywords.csv')\n",
    "keywords.head()\n",
    "keywords['public transport']=keywords['public transport'].str.lower()\n",
    "keywords['motorized']=keywords['motorized'].str.lower()\n",
    "keywords['non-motorized']=keywords['non-motorized'].str.lower()\n",
    "\n",
    "transit = keywords['public transport'].tolist()\n",
    "motorized = keywords['motorized'].tolist()\n",
    "nonmotorized = keywords['non-motorized'].tolist()\n",
    "\n",
    "keywords = set(transit + motorized + nonmotorized)\n",
    "\n",
    "print(keywords)\n",
    "print(len(keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0357d5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "def containsKeyword(text, keywords):\n",
    "    '''\n",
    "    Checks if a word in the tweet body is located in the keywords list.\n",
    "    '''    \n",
    "    \n",
    "    loweredText = [word.lower() for word in text.split()]\n",
    "    textSet = set(loweredText) \n",
    "    \n",
    "    intersection = textSet & keywords\n",
    "    \n",
    "    return False if not intersection else True\n",
    "\n",
    "print(containsKeyword(\"hi there car subaru drive\", keywords))\n",
    "print(containsKeyword(\"Creating a Grocery List Manager Using Angular, Part 1: Add &amp; Display Items https://t.co/xFox78juL1 #Angular\", keywords))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513931e2",
   "metadata": {},
   "source": [
    "### Save the date range of the transportation tweets saved in each file <a href name id=\"2c\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41be4ba",
   "metadata": {},
   "source": [
    "First, we generate functions to patternize the file names. Each `.jsonl` file will be prefixed with `covid-mobility-tweets-starting` followed by the date of the first tweet in the file.\n",
    "\n",
    "Template: `covid-mobility-tweets-starting-YYYY-MM-DD.jsonl`\n",
    "\n",
    "Example: `covid-mobility-tweets-starting-2020-05-01.jsonl`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "601d7bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***TESTS***\n",
      "Year Month Day 2018 10 10 20:19:24\n",
      "Tweet File Name:  covid-mobility-tweet-starting-2020-5-1-0.jsonl\n",
      "Date Range: 2020-5-1-0 00:11:52 to 2020-5-1 00:11:52\n"
     ]
    }
   ],
   "source": [
    "# test datetime object format\n",
    "date_time_str = \"Wed Oct 10 20:19:24 +0000 2018\"\n",
    "date_time_obj = datetime.datetime.strptime(date_time_str, '%a %b %d %H:%M:%S %z %Y')\n",
    "time = date_time_obj.strftime(\"%H:%M:%S\")\n",
    "print(\"***TESTS***\")\n",
    "print(\"Year Month Day\", date_time_obj.year, date_time_obj.month, date_time_obj.day, time)\n",
    "\n",
    "# note: I removed some keys for brevity\n",
    "sampleTweet = {\n",
    "    'created_at': 'Fri May 01 00:11:52 +0000 2020',\n",
    " 'id': 1256013431855108096,\n",
    " 'id_str': '1256013431855108096',\n",
    " 'text': 'RT @VEJA: Covid-19 avança nas favelas — mas ruas continuam cheias com gente que não pode parar e se proteger https://t.co/rPk8eequN5 https:…',\n",
    " 'truncated': False,\n",
    " 'entities': {'hashtags': [],\n",
    "  'symbols': [],\n",
    "  'user_mentions': [{'screen_name': 'VEJA',\n",
    "    'name': 'VEJA',\n",
    "    'id': 17715048,\n",
    "    'id_str': '17715048',\n",
    "    'indices': [3, 8]}],\n",
    "  'urls': [{'url': 'https://t.co/rPk8eequN5',\n",
    "    'expanded_url': 'https://veja.abril.com.br/brasil/o-coronavirus-chega-a-favela/',\n",
    "    'display_url': 'veja.abril.com.br/brasil/o-coron…',\n",
    "    'indices': [109, 132]}]},\n",
    " 'source': '<a href=\"https://mobile.twitter.com\" rel=\"nofollow\">Twitter Web App</a>',\n",
    " 'geo': None,\n",
    " 'coordinates': None,\n",
    " 'place': None,\n",
    " 'contributors': None,\n",
    "}\n",
    "\n",
    "# Test helper functions\n",
    "print(\"Tweet File Name: \", formatFileName([sampleTweet]))\n",
    "print(fetchStartAndEndDate([sampleTweet], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f58dbbab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'covid-mobility-tweet-starting-2020-5-1-0.jsonl'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# helper functions used for saveTransportationTweets()\n",
    "def fetchStartAndEndDate(tweetArr, count):\n",
    "    '''\n",
    "    Returns the first and last tweet in the dataset\n",
    "    in the format YEAR-MONTH-DATE-HOUR to YEAR-MONTH-DATE-HOUR \n",
    "    '''\n",
    "    startTimeStr = tweetArr[0][\"created_at\"]\n",
    "    s = datetime.datetime.strptime(startTimeStr, '%a %b %d %H:%M:%S %z %Y')\n",
    "    sTime = s.strftime(\"%H:%M:%S\")\n",
    "    # print(\"s\",sTime)\n",
    "    endTimeStr = tweetArr[count-1][\"created_at\"]\n",
    "    e = datetime.datetime.strptime(endTimeStr, '%a %b %d %H:%M:%S %z %Y')\n",
    "    eTime = e.strftime(\"%H:%M:%S\")\n",
    "    # print(\"e\",eTime)\n",
    "\n",
    "    dateRange = f'Date Range: {s.year}-{s.month}-{s.day}-{s.hour} {sTime} to {e.year}-{e.month}-{e.day} {eTime}'\n",
    "    \n",
    "    return dateRange \n",
    "\n",
    "def formatFileName(tweetArr):\n",
    "    '''\n",
    "    Formats the file name of the json files according to the first and last tweet in the dataset\n",
    "    using the prefix \"covid-mobility-tweet-starting\" followed by YEAR-MONTH-DATE~HH-mm-ss.\n",
    "    '''\n",
    "    startTimeStr = tweetArr[0][\"created_at\"]\n",
    "    s = datetime.datetime.strptime(startTimeStr, '%a %b %d %H:%M:%S %z %Y')\n",
    "    sTime = s.strftime(\"%H-%M-%S\")\n",
    "\n",
    "    base_path = f'covid-mobility-tweet-starting-{s.year}-{s.month}-{s.day}'\n",
    "    filename_0 = base_path+'-0'\n",
    "    \n",
    "    if not os.path.exists(f'{filename_0}.jsonl'): # path with this date does not exist yet\n",
    "        return filename_0+'.jsonl'\n",
    "    else:    \n",
    "        # save file as the next `filename-i` filepath for this date\n",
    "        i = 1\n",
    "        while os.path.exists(f'{base_path}-{i}.jsonl'):\n",
    "            i+=1\n",
    "        filename_i = f'{base_path}-{i}'\n",
    "    \n",
    "        return filename_i+'.jsonl'\n",
    "        \n",
    "\n",
    "def writeToJsonlFile(tweetArr, dateRange, filename):\n",
    "    '''\n",
    "    Writes a list of tweets to a given .jsonl file. Note that .jsonl and .json differ in structure.\n",
    "    '''\n",
    "    filepath = './covid-mobility-tweets/'+filename\n",
    "    with open(filepath, 'w') as outfile:\n",
    "        outfile.write(f'{{{dateRange}}}\\n') # dumps the date range in json format\n",
    "        for entry in tweetArr:\n",
    "            json.dump(entry, outfile)\n",
    "            outfile.write('\\n')\n",
    "\n",
    "formatFileName([sampleTweet])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ac870a",
   "metadata": {},
   "source": [
    "The next few functions make up the core of the data collection. The `saveTransportationTweets()` function iterates through the list of tweets given an array of tweet IDs, adding tweets related to our transportation keywords to an transportation tweet array. When the size of the transportation tweet array reaches 1000, we save the file as a `.jsonl` file and reiterate through the process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d2636d",
   "metadata": {},
   "source": [
    "### Define functions to fetch and save `COVID-19+Transportation` data. <a href name id='2d'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10181efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveTransportationTweets(tweetIDs, keywords):\n",
    "    '''\n",
    "    Iterates through list of tweets and identifies tweets containing keywords. \n",
    "    \n",
    "    When the number of transportation tweets surpasses 1000, save the current tweets \n",
    "    as a json file and reset the json variable.\n",
    "    '''\n",
    "    tweetArr = []\n",
    "    count = 0\n",
    "    for tweet in tweetIDs:\n",
    "        # save json file and create new one\n",
    "        if count == 1000:\n",
    "            dateRange = fetchStartAndEndDate(tweetArr, count) # save date of first and last tweet\n",
    "            filename = formatFileName(tweetArr) # format file name\n",
    "            writeToJsonlFile(tweetArr, dateRange, filename) # save json as a json file\n",
    "            # reset 'global' vars\n",
    "            tweetArr = []\n",
    "            count = 0\n",
    "        try:\n",
    "            fetchedTweet = api.get_status(tweet)\n",
    "            \n",
    "            # search for keywords in tweet text\n",
    "            if containsKeyword(fetchedTweet.text, keywords):\n",
    "                tweetArr.append(fetchedTweet._json)\n",
    "                count += 1 # maintain counter to know when the json file reaches 1000 COVID-19+Transportation tweets\n",
    "                print(f\"COVID-19+Transportation Tweet #{count}\\n\" + fetchedTweet.text)\n",
    "\n",
    "        except Exception as e:\n",
    "            # print(\"Exception Thrown: \", e)\n",
    "            continue\n",
    "    \n",
    "    # if loop exits befor count == 1000\n",
    "    if count > 0:\n",
    "        dateRange = fetchStartAndEndDate(tweetArr, count) # save date of first and last tweet\n",
    "        filename = formatFileName(tweetArr) # format file name\n",
    "        writeToJsonlFile(tweetArr, dateRange, filename) # save json as a json file    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a872e71d",
   "metadata": {},
   "source": [
    "## Finally, it's time to call the Twitter API <a href name id='3'>\n",
    "    \n",
    "Using the functions we defined above, we are now ready to call `saveTransportationTweets()` on the COVID-19 dataset files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "397d2c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files in COVID-10 Dataset: 19586\n",
      "The first 3 automatically generated files: ['https://raw.githubusercontent.com/echen102/COVID-19-TweetIDs/master/2020-01/coronavirus-tweet-id-2020-01-21-22.txt', 'https://raw.githubusercontent.com/echen102/COVID-19-TweetIDs/master/2020-01/coronavirus-tweet-id-2020-01-21-23.txt', 'https://raw.githubusercontent.com/echen102/COVID-19-TweetIDs/master/2020-01/coronavirus-tweet-id-2020-01-22-00.txt']\n"
     ]
    }
   ],
   "source": [
    "time_range = ['2020-01', '2020-02', '2020-03', '2020-04', '2020-05', '2020-06', '2020-07', '2020-08', '2020-09', '2020-10', '2020-11', '2020-12', \n",
    "            '2021-01', '2021-02', '2021-03', '2021-04', '2021-05', '2021-06', '2021-07', '2021-08', '2021-09', '2021-10', '2021-11', '2021-12', \n",
    "            '2022-01', '2022-02', '2022-03']\n",
    "\n",
    "covidDataset = fetchAllTextFileURLs(time_range)\n",
    "covidDataset = covidDataset[502:] # data collection starts from 1/21/2020 at 22:00.\n",
    "print('Total files in COVID-10 Dataset:', len(covidDataset))\n",
    "# show first 5 txt files\n",
    "print('The first 3 automatically generated files:', covidDataset[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310e5120",
   "metadata": {},
   "source": [
    "The following lines of code do \n",
    "- selects several .txt files to extract tweets from\n",
    "- extracts tweet from each .txt file\n",
    "- runs `saveTransportationTweets()` which rehydrates the Tweet object and saves the relevant tweets in .jsonl files\n",
    "    - we define 'relevant' using the [keywords](#2b) defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d78bbfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selectedRange = covidDataset[8:15] # note that 1/21 happens at 24 * 21 = 504\n",
    "# for file in selectedRange:\n",
    "#     tweetIDs = getTweetIDsFrom(file)\n",
    "#     if len(tweetIDs) > 0:\n",
    "#         print('tweetIDs found')\n",
    "#         saveTransportationTweets(tweetIDs, keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9271d856",
   "metadata": {},
   "source": [
    "### Dehydrate `jsonl` files back to `txt` file of tweet IDs  <a href name id='4'>\n",
    "\n",
    "Now that we have our dataset of `COVID-19+Transportation` tweets, we can test out Twarc's `dehydrate` function to compress the tweets into just tweet IDs. Twarc is also a command line too, which means we can run \n",
    "\n",
    "    twarc dehydrate covid-mobility-tweet-starting-2020-1-21.jsonl > covid-mobility-tweet-starting-2020-1-21.txt\n",
    "    \n",
    "by utilizing the `os` library. According to the documentation, Twarc generates a text file containing the tweet IDs it finds based on the key `id_str`. The [source code](https://twarc-project.readthedocs.io/en/latest/api/client/) (for the Client method) is written below:\n",
    "    \n",
    "\n",
    "    def dehydrate(self, iterator):\n",
    "    \"\"\"\n",
    "    Pass in an iterator of tweets' JSON and get back an iterator of the\n",
    "    IDs of each tweet.\n",
    "    \"\"\"\n",
    "    for line in iterator:\n",
    "        try:\n",
    "            yield json.loads(line)[\"id_str\"]\n",
    "        except Exception as e:\n",
    "            log.error(\"uhoh: %s\\n\" % e)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a46a086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Twarc; not currently used in client\n",
    "twarc = Twarc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f91b7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dehydrate on a sample .jsonl file\n",
    "os.system('twarc dehydrate covid-mobility-tweet-starting-2020-1-21.jsonl > covid-mobility-tweet-starting-2020-1-21.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9da5813",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./covid-mobility-tweet-starting-2020-1-21.txt \n",
      "\n",
      "Tweet IDs in ./covid-mobility-tweet-starting-2020-1-21.txt\n"
     ]
    }
   ],
   "source": [
    "def findfile(name, path):\n",
    "    '''\n",
    "    Finds file in directory\n",
    "    '''\n",
    "    for dirpath, dirname, filename in os.walk(path):\n",
    "        if name in filename:\n",
    "            return os.path.join(dirpath, name)\n",
    "\n",
    "filepath = findfile('covid-mobility-tweet-starting-2020-1-21.txt', \"./\")\n",
    "print(filepath, '\\n')\n",
    "\n",
    "with open(filepath, 'r') as file:\n",
    "    print(f\"Tweet IDs in {filepath}\")\n",
    "    for line in file:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6568b13",
   "metadata": {},
   "source": [
    "## Update: Data Collection through `general-collector.py` (4/7/2022) <a href name id='5'>\n",
    "    \n",
    "To collect data on the remote CS server, I reformatted my Jupyter Notebook into a Python script. I've been using the `nohup` command on Linux to constantly collect data from the COVID-19 dataset. Because of the difference between the notebook version and the script, I have had to make a few changes to this notebook, particularly in relation to the naming conventions. I've also uploaded my data collection script (`dataCollection.py`) in my `finalproject` folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35a64ce",
   "metadata": {},
   "source": [
    "When creating new files:\n",
    "1. Follow conventions to generate python file `collect-MMDDYYYY-MMDDYYYY.py`\n",
    "2. Create folder with the same format `collect-MMDDYYYY-MMDDYYYY`\n",
    "3. Copy and older python file. Change the following:\n",
    "    1. Ctrl-f `collect-`. Change two log files and the folder path to the proper name\n",
    "    1. Ctrl-f `.log\"`. Check two log files have been changed\n",
    "    2. Ctrl-f `selectedRange`. Change the selectedRange to the date range we're collecting from\n",
    "\n",
    "Date Ranges currently being collected from:\n",
    "- 3/11/2020 - 3/27/2020\n",
    "- 3/28/2020 - 5/1/2020\n",
    "- 5/2/2020 - 5/8/2020\n",
    "- 7/23/2020 - 8/30/2020\n",
    "\n",
    "- 11/24/2020 - 11/31/2020\n",
    "- 12/11/2020 - 12/25/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0552b9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the general collector script\n",
    "import os\n",
    "\n",
    "def startGeneralCollector(collect_dateRange, startIndex, endIndex):\n",
    "    scriptCommand = '../miniconda3/bin/python3 collection-scripts/general-collector.py'\n",
    "    os.system(f'{scriptCommand} --collect_dateRange {collect_dateRange} --startIndex {startIndex} --endIndex {endIndex}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9f8813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# startGeneralCollector('collect-06072021-06132021', 12290, 12458)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b0ac20",
   "metadata": {},
   "source": [
    "I used [this spreadsheet](https://docs.google.com/spreadsheets/d/1hKOxDP_hTEeD7rBBV-W_REfw2Dt1pYwiJwfYwSy5u8Q/edit?usp=sharing) to keep track of the data collection periods and ensure I did not double collect data. There was no time to waste on silly mistakes (although I sure did make a few...)! The `general-collector` script was very helpful in streamlining the data collection process. The abstraction + spreadsheet combined allowed me to work 10x faster!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a963e016",
   "metadata": {},
   "source": [
    "## Other tests [can ignore]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8166cd",
   "metadata": {},
   "source": [
    "As mentioned previously, our script creates \"urls\" between 1/2020 and the current month (4/2022) from day 1 to 31 and hour 0 to 23. We used this to simplify the url generation process when accessing the COVID-19 Datset Github Repo. Because not all months have 31 days, and not all hours have collected data, we have an additional check that ensures we ignore false links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b1d7968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "January 2020 ['2020', '01', '21', '22.txt'] ['2020', '01', '31', '23.txt']\n",
      "February 2020 ['2020', '02', '01', '00.txt'] ['2020', '02', '31', '23.txt']\n",
      "March 2020 ['2020', '03', '01', '00.txt'] ['2020', '03', '31', '23.txt']\n",
      "April 2020 ['2020', '04', '01', '00.txt'] ['2020', '04', '31', '23.txt']\n"
     ]
    }
   ],
   "source": [
    "# Get an idea of the month to month breakdown\n",
    "\n",
    "# Data collection for the mont of January, 2020 ran from 1/21/2020 to 1/31/2020, \n",
    "# the 502th index to the 743 index of our covid dataset\n",
    "print(\"January 2020\", covidDataset[0].split('-')[6:], covidDataset[241].split('-')[6:])\n",
    "\n",
    "# We iterate over all 24 hours and all '31' days (regardless of month)\n",
    "# To find the entire month of data, we look for covidDataset[744(X-2)+241, 744(X-1)+241+1] where X = month #\n",
    "print(\"February 2020\", covidDataset[242].split('-')[6:], covidDataset[985].split('-')[6:])\n",
    "\n",
    "print(\"March 2020\", covidDataset[986].split('-')[6:], covidDataset[1729].split('-')[6:])\n",
    "\n",
    "print(\"April 2020\", covidDataset[1730].split('-')[6:], covidDataset[2473].split('-')[6:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c069b986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://raw.githubusercontent.com/echen102/COVID-19-TweetIDs/master/2020-01/coronavirus-tweet-id-2020-01-25-04.txt'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covidDataset[78]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "47793ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19586\n",
      "https://raw.githubusercontent.com/echen102/COVID-19-TweetIDs/master/2020-12/coronavirus-tweet-id-2020-12-11-00.txt\n",
      "['https://raw.githubusercontent.com/echen102/COVID-19-TweetIDs/master/2020-12/coronavirus-tweet-id-2020-12-25-23.txt']\n"
     ]
    }
   ],
   "source": [
    "print(len(covidDataset))\n",
    "print(covidDataset[7922])\n",
    "# save the second value bc it's index-1\n",
    "print(covidDataset[8281:8282])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d1f0f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
